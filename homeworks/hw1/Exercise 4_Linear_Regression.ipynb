{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OZ-7Uhprk6N"
      },
      "source": [
        "# Intro to Python: Exercise 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-e4lXfnrk6S"
      },
      "source": [
        "## Linear regression with one variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6I29obGrk6T"
      },
      "source": [
        "In the first part of the exercise, we're tasked with implementing linear regression with one variable to predict profits for a food truck. Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. The chain already has trucks in various cities and you have data for profits and populations from the cities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwbI6Rk-rk6U"
      },
      "source": [
        "Let's start by importing some libraries and examining the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HCPDnogBrk6V"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwrO8IlHrk6X"
      },
      "source": [
        "Read the data from the CSV file using Panda library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1E9pKJuNrk6X",
        "outputId": "93534bc4-6c0f-4efa-fca0-ded8b7ec6988",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLAV-2022'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 36 (delta 11), reused 26 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-55364b92-9e1e-4521-9024-8661bea747fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Population</th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.1101</td>\n",
              "      <td>17.5920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.5277</td>\n",
              "      <td>9.1302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.5186</td>\n",
              "      <td>13.6620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.0032</td>\n",
              "      <td>11.8540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.8598</td>\n",
              "      <td>6.8233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55364b92-9e1e-4521-9024-8661bea747fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-55364b92-9e1e-4521-9024-8661bea747fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-55364b92-9e1e-4521-9024-8661bea747fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Population   Profit\n",
              "0      6.1101  17.5920\n",
              "1      5.5277   9.1302\n",
              "2      8.5186  13.6620\n",
              "3      7.0032  11.8540\n",
              "4      5.8598   6.8233"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "!git clone https://github.com/vita-epfl/DLAV-2022.git\n",
        "path = os.getcwd() + '/DLAV-2022/homeworks/hw1/data/ex1data1.txt'\n",
        "data = pd.read_csv(path, header=None, names=['Population', 'Profit'])\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CQvZNE6irk6Y",
        "outputId": "dfb2cf18-503f-49c1-8233-ab0256c2e1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f86c88a7-feaa-4abd-a82c-bb15af86402c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Population</th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>97.000000</td>\n",
              "      <td>97.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>8.159800</td>\n",
              "      <td>5.839135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.869884</td>\n",
              "      <td>5.510262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.026900</td>\n",
              "      <td>-2.680700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>5.707700</td>\n",
              "      <td>1.986900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.589400</td>\n",
              "      <td>4.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.578100</td>\n",
              "      <td>7.046700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>22.203000</td>\n",
              "      <td>24.147000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f86c88a7-feaa-4abd-a82c-bb15af86402c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f86c88a7-feaa-4abd-a82c-bb15af86402c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f86c88a7-feaa-4abd-a82c-bb15af86402c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       Population     Profit\n",
              "count   97.000000  97.000000\n",
              "mean     8.159800   5.839135\n",
              "std      3.869884   5.510262\n",
              "min      5.026900  -2.680700\n",
              "25%      5.707700   1.986900\n",
              "50%      6.589400   4.562300\n",
              "75%      8.578100   7.046700\n",
              "max     22.203000  24.147000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkX3rk5yrk6Z"
      },
      "source": [
        "Let's plot it to get a better idea of what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLNYSWkUrk6a",
        "outputId": "03697b25-2f82-4399-c04a-775bcb2a11f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdc9dd9b050>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "data.plot(kind='scatter', x='Population', y='Profit', figsize=(12,8))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxDs41Xark6b"
      },
      "source": [
        "Now let's implement linear regression using gradient descent to minimize the cost function.  The equations implemented in the following code samples are detailed in \"ex1.pdf\" in the \"exercises\" folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9MoUzkqrk6c"
      },
      "source": [
        "First we'll create a function to compute the cost of a given solution (characterized by the parameters theta). The cost function is the Mean Sqaured error in matrix form: \n",
        "\n",
        "$$ MSE(\\theta) = \\frac{1}{N}\\sum_n^N [ y_n-x_n^T*\\theta]^2 $$\n",
        "\n",
        "where $\\theta$ and $x_n$ are vectors\n",
        "\n",
        "__Hint__: Use the matrix form of the cost function and make use of numpy operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5w9x4Higrk6c"
      },
      "outputs": [],
      "source": [
        "def computeCost(x, y, theta):\n",
        "    N = y.shape[0]\n",
        "    return np.linalg.norm(y-x@theta,ord=2,axis=0)**2/N"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvXi0aekrk6d"
      },
      "source": [
        "Let's add a column of ones to the training set so we can use a vectorized solution to computing the cost and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MVUbDWhHrk6d"
      },
      "outputs": [],
      "source": [
        "data.insert(0, 'Ones', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiDSRC0yrk6d"
      },
      "source": [
        "Now let's do some variable initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uV4dwW1irk6e"
      },
      "outputs": [],
      "source": [
        "# set X (training data) and y (target variable)\n",
        "cols = data.shape[1]\n",
        "X = data.iloc[:,0:cols-1]\n",
        "y = data.iloc[:,cols-1:cols]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr_BtL1wrk6e"
      },
      "source": [
        "Let's take a look to make sure X (training set) and y (target variable) look correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEDH-dHVrk6f",
        "outputId": "508b7117-41f7-49d6-a7e7-b32eda2dc0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8370a514-c212-48f4-a13d-bef5fc1255d1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ones</th>\n",
              "      <th>Population</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>6.1101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5.5277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>8.5186</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>7.0032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5.8598</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8370a514-c212-48f4-a13d-bef5fc1255d1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8370a514-c212-48f4-a13d-bef5fc1255d1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8370a514-c212-48f4-a13d-bef5fc1255d1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Ones  Population\n",
              "0     1      6.1101\n",
              "1     1      5.5277\n",
              "2     1      8.5186\n",
              "3     1      7.0032\n",
              "4     1      5.8598"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilHRU-lark6f",
        "outputId": "784d8ade-928e-4520-c757-f2aaa43a40a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bdfdcea5-9928-4487-8418-229fc1a1b73c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Profit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.5920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.1302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.6620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.8540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.8233</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bdfdcea5-9928-4487-8418-229fc1a1b73c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bdfdcea5-9928-4487-8418-229fc1a1b73c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bdfdcea5-9928-4487-8418-229fc1a1b73c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    Profit\n",
              "0  17.5920\n",
              "1   9.1302\n",
              "2  13.6620\n",
              "3  11.8540\n",
              "4   6.8233"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gW9kvvUrk6f"
      },
      "source": [
        "Convert X and Y to numpy array for better manipulation. Initiliaze Theta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZqZO8SCerk6g",
        "outputId": "6c4a5051-e6ba-4252-e607-2ac80e327184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X = np.array(X.values)\n",
        "y = np.array(y.values).flatten()\n",
        "theta = np.array([0,0])\n",
        "theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9Cx_H5Prk6g"
      },
      "source": [
        "Let's take a quick look at the shape of our matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zqvv6Y4Trk6h",
        "outputId": "c9115bf7-7148-4021-a915-53c4c1664b34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((97, 2), (2,), (97,))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "X.shape, theta.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIwo8t3jrk6h"
      },
      "source": [
        "Now let's compute the cost for our initial solution (0 values for theta)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kSirxzvLrk6h",
        "outputId": "e2df5665-e6a6-4858-9846-8992cec9c363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.14546775491135"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "computeCost(X, y, theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-iHYTlbrk6h"
      },
      "source": [
        "So far so good.  Now we need to define a function to perform gradient descent on the parameters theta using the update rules. Write first a function that computes the gradient of a matrix and then use it in the gradientDescent function.\n",
        "\n",
        "The gradient descent formula is:\n",
        "\n",
        "$$\\theta^{t+1} = \\theta^{t} - \\alpha*\\nabla MSE(\\theta^{t})$$\n",
        "\n",
        "where $\\nabla MSE(\\theta^{t})$ is the gradient of the cost function at $\\theta^{t}$\n",
        "\n",
        "__Hint__: Use the matrix form of the gradient and make use of numpy operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ionYgLttrk6i"
      },
      "outputs": [],
      "source": [
        "def compute_gradient(y, tx, w):\n",
        "    \"\"\"Compute the gradient.\"\"\"\n",
        "    N = y.shape[0]\n",
        "    grad = 2/N*np.sum(-tx@(y-tx.T@w),axis=0)\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "mPI15Nferk6i"
      },
      "outputs": [],
      "source": [
        "def gradientDescent(X, y, theta, alpha,max_iters):\n",
        "    \"\"\"Gradient descent algorithm.\"\"\"\n",
        "    # Define parameters to store w and loss\n",
        "    ws = [theta]\n",
        "    cost = np.zeros(max_iters)\n",
        "    for n_iter in range(max_iters):\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: compute gradient and loss\n",
        "        # ***************************************************\n",
        "        grad = compute_gradient(y,X.T,theta)\n",
        "        loss = computeCost(X,y,theta)\n",
        "        # ***************************************************\n",
        "        # INSERT YOUR CODE HERE\n",
        "        # TODO: update theta by gradient\n",
        "        # ***************************************************\n",
        "        theta = theta - alpha*grad\n",
        "        # store w and loss\n",
        "        ws.append(theta)\n",
        "        cost[n_iter] = loss\n",
        "        print(\"Gradient Descent({bi}/{ti}): loss={l}, w0={w0}, w1={w1}\".format(\n",
        "              bi=n_iter, ti=max_iters - 1, l=loss, w0=theta[0], w1=theta[1]))\n",
        "\n",
        "    return theta, cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06lqe-ofrk6j"
      },
      "source": [
        "Initialize some additional variables - the learning rate alpha, and the number of iterations to perform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ov0dKjolrk6j"
      },
      "outputs": [],
      "source": [
        "alpha = 0.01\n",
        "iters = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLrtEvpCrk6j"
      },
      "source": [
        "Now let's run the gradient descent algorithm to fit our parameters theta to the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NhIvsW4irk6k",
        "outputId": "4aaf01ce-f1f1-44eb-a5be-87f313b669e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Descent(0/999): loss=64.14546775491135, w0=1.4233596959420618, w1=1.4233596959420618\n",
            "Gradient Descent(1/999): loss=61.55942573051127, w0=0.03633715401346227, w1=0.03633715401346227\n",
            "Gradient Descent(2/999): loss=59.10373716257835, w0=1.3879501983499263, w1=1.3879501983499263\n",
            "Gradient Descent(3/999): loss=56.77183138309846, w0=0.07084267745822403, w1=0.07084267745822403\n",
            "Gradient Descent(4/999): loss=54.55746892876287, w0=1.3543255713667561, w1=1.3543255713667561\n",
            "Gradient Descent(5/999): loss=52.45472484608181, w0=0.1036088965120312, w1=0.1036088965120312\n",
            "Gradient Descent(6/999): loss=50.45797283802949, w0=1.322395845823051, w1=1.322395845823051\n",
            "Gradient Descent(7/999): loss=48.561870209802144, w0=0.13472348350853935, w1=0.13472348350853935\n",
            "Gradient Descent(8/999): loss=46.76134357340863, w0=1.2920755875847485, w1=1.2920755875847485\n",
            "Gradient Descent(9/999): loss=45.05157527284277, w0=0.16426969152153736, w1=0.16426969152153736\n",
            "Gradient Descent(10/999): loss=43.42799049351669, w0=1.2632836689577316, w1=1.2632836689577316\n",
            "Gradient Descent(11/999): loss=41.886245021462365, w0=0.19232657712459322, w1=0.19232657712459322\n",
            "Gradient Descent(12/999): loss=40.42221361955019, w0=1.2359430516150482, w1=1.2359430516150482\n",
            "Gradient Descent(13/999): loss=39.03197898962208, w0=0.21896921192214958, w1=0.21896921192214958\n",
            "Gradient Descent(14/999): loss=37.71182129100503, w0=1.2099805804660222, w1=1.2099805804660222\n",
            "Gradient Descent(15/999): loss=36.4582081873608, w0=0.24426888341807207, w1=0.24426888341807207\n",
            "Gradient Descent(16/999): loss=35.26778539523923, w0=1.1853267879157086, w1=1.1853267879157086\n",
            "Gradient Descent(17/999): loss=34.137367709046394, w0=0.2682932857591006, w1=0.2682932857591006\n",
            "Gradient Descent(18/999): loss=33.06393047841327, w0=1.1619157079909574, w1=1.1619157079909574\n",
            "Gradient Descent(19/999): loss=32.04460151516066, w0=0.29110670086358714, w1=0.29110670086358714\n",
            "Gradient Descent(20/999): loss=31.076653408206354, w0=1.139684699835739, w1=1.139684699835739\n",
            "Gradient Descent(21/999): loss=30.15749622585103, w0=0.3127701704201551, w1=0.3127701704201551\n",
            "Gradient Descent(22/999): loss=29.284670585916942, w0=1.118574280103462, w1=1.118574280103462\n",
            "Gradient Descent(23/999): loss=28.455841075196968, w0=0.33334165921650105, w1=0.33334165921650105\n",
            "Gradient Descent(24/999): loss=27.668790000606364, w0=1.0985279637978151, w1=1.0985279637978151\n",
            "Gradient Descent(25/999): loss=26.92141145531771, w0=0.35287621023535665, w1=0.35287621023535665\n",
            "Gradient Descent(26/999): loss=26.211705684001217, w0=1.079492113136276, w1=1.079492113136276\n",
            "Gradient Descent(27/999): loss=25.53777373209405, w0=0.3714260919325941, w1=0.3714260919325941\n",
            "Gradient Descent(28/999): loss=24.897812364781338, w0=1.061415794031885, w1=1.061415794031885\n",
            "Gradient Descent(29/999): loss=24.290109242094008, w0=0.38904093809155527, w1=0.38904093809155527\n",
            "Gradient Descent(30/999): loss=23.71303833721319, w0=1.0442506398092815, w1=1.0442506398092815\n",
            "Gradient Descent(31/999): loss=23.165055585722012, w0=0.40576788062780156, w1=0.40576788062780156\n",
            "Gradient Descent(32/999): loss=22.6446947541638, w0=1.0279507217903423, w1=1.0279507217903423\n",
            "Gradient Descent(33/999): loss=22.150563516851825, w0=0.421651675699639, w1=0.421651675699639\n",
            "Gradient Descent(34/999): loss=21.681339730433645, w0=1.012472426403152, w1=1.012472426403152\n",
            "Gradient Descent(35/999): loss=21.235767896241722, w0=0.436734823461845, w1=0.436734823461845\n",
            "Gradient Descent(36/999): loss=20.812655800964823, w0=0.9977743384854851, w1=0.9977743384854851\n",
            "Gradient Descent(37/999): loss=20.410871326651534, w0=0.4510576817830223, w1=0.4510576817830223\n",
            "Gradient Descent(38/999): loss=20.02933942151053, w0=0.9838171304705611, w1=0.9838171304705611\n",
            "Gradient Descent(39/999): loss=19.66703922340226, w0=0.464658574230854, w1=0.464658574230854\n",
            "Gradient Descent(40/999): loss=19.32300132832563, w0=0.9705634571585576, w1=0.9705634571585576\n",
            "Gradient Descent(41/999): loss=18.996305196590754, w0=0.4775738926141942, w1=0.4775738926141942\n",
            "Gradient Descent(42/999): loss=18.686076689737543, w0=0.9579778557923326, w1=0.9579778557923326\n",
            "Gradient Descent(43/999): loss=18.39148573160991, w0=0.48983819435636755, w1=0.48983819435636755\n",
            "Gradient Descent(44/999): loss=18.11174408732698, w0=0.9460266511699833, w1=0.9460266511699833\n",
            "Gradient Descent(45/999): loss=17.84610325420877, w0=0.5014842949602127, w1=0.5014842949602127\n",
            "Gradient Descent(46/999): loss=17.593852459013007, w0=0.9346778655403534, w1=0.9346778655403534\n",
            "Gradient Descent(47/999): loss=17.354316756124362, w0=0.5125433558122863, w1=0.5125433558122863\n",
            "Gradient Descent(48/999): loss=17.1268552216074, w0=0.9239011330403976, w1=0.9239011330403976\n",
            "Gradient Descent(49/999): loss=16.910859238291007, w0=0.5230449675611561, w1=0.5230449675611561\n",
            "Gradient Descent(50/999): loss=16.705750867295944, w0=0.9136676184454628, w1=0.9136676184454628\n",
            "Gradient Descent(51/999): loss=16.51098130164792, w0=0.5330172292928862, w1=0.5330172292928862\n",
            "Gradient Descent(52/999): loss=16.326029397838823, w0=0.9039499400150879, w1=0.9039499400150879\n",
            "Gradient Descent(53/999): loss=16.150400281406817, w0=0.5424868237155542, w1=0.5424868237155542\n",
            "Gradient Descent(54/999): loss=15.983624022804333, w0=0.8947220962278792, w1=0.8947220962278792\n",
            "Gradient Descent(55/999): loss=15.825254380011017, w0=0.5514790885539814, w1=0.5514790885539814\n",
            "Gradient Descent(56/999): loss=15.674867604527194, w0=0.8859593962094309, w1=0.8859593962094309\n",
            "Gradient Descent(57/999): loss=15.532061307553127, w0=0.5600180843456963, w1=0.5600180843456963\n",
            "Gradient Descent(58/999): loss=15.396453383320178, w0=0.8776383936671301, w1=0.8776383936671301\n",
            "Gradient Descent(59/999): loss=15.267680986693243, w0=0.5681266588195419, w1=0.5681266588195419\n",
            "Gradient Descent(60/999): loss=15.145399562308636, w0=0.8697368241550844, w1=0.8697368241550844\n",
            "Gradient Descent(61/999): loss=15.029281922649828, w0=0.5758265080291751, w1=0.5758265080291751\n",
            "Gradient Descent(62/999): loss=14.919017372594173, w0=0.8622335455013046, w1=0.8622335455013046\n",
            "Gradient Descent(63/999): loss=14.814310878088238, w0=0.5831382344050413, w1=0.5831382344050413\n",
            "Gradient Descent(64/999): loss=14.714882276727359, w0=0.8551084812377503, w1=0.8551084812377503\n",
            "Gradient Descent(65/999): loss=14.62046552812711, w0=0.5900814018801458, w1=0.5900814018801458\n",
            "Gradient Descent(66/999): loss=14.530808002080981, w0=0.8483425668818703, w1=0.8483425668818703\n",
            "Gradient Descent(67/999): loss=14.445669802599587, w0=0.5966745882371283, w1=0.5966745882371283\n",
            "Gradient Descent(68/999): loss=14.364823126022731, w0=0.841917698925905, w1=0.841917698925905\n",
            "Gradient Descent(69/999): loss=14.28805165148677, w0=0.6029354348166996, w1=0.6029354348166996\n",
            "Gradient Descent(70/999): loss=14.215149962116483, w0=0.8358166863974629, w1=0.8358166863974629\n",
            "Gradient Descent(71/999): loss=14.145922995392635, w0=0.6088806937204493, w1=0.6088806937204493\n",
            "Gradient Descent(72/999): loss=14.080185521224616, w0=0.8300232048617611, w1=0.8300232048617611\n",
            "Gradient Descent(73/999): loss=14.01776164633161, w0=0.6145262726343211, w1=0.6145262726343211\n",
            "Gradient Descent(74/999): loss=13.958484343606267, w0=0.8245217527424537, w1=0.8245217527424537\n",
            "Gradient Descent(75/999): loss=13.902195005201454, w0=0.6198872773926931, w1=0.6198872773926931\n",
            "Gradient Descent(76/999): loss=13.848743018144406, w0=0.8192976098441774, w1=0.8192976098441774\n",
            "Gradient Descent(77/999): loss=13.7979853613427, w0=0.6249780523969477, w1=0.6249780523969477\n",
            "Gradient Descent(78/999): loss=13.749786222903749, w0=0.8143367979658318, w1=0.8143367979658318\n",
            "Gradient Descent(79/999): loss=13.704016636743901, w0=0.629812218996683, w1=0.629812218996683\n",
            "Gradient Descent(80/999): loss=13.660554137514866, w0=0.8096260434992082, w1=0.8096260434992082\n",
            "Gradient Descent(81/999): loss=13.619282432924022, w0=0.6344027119362582, w1=0.6344027119362582\n",
            "Gradient Descent(82/999): loss=13.580091092572017, w0=0.8051527419128928, w1=0.8051527419128928\n",
            "Gradient Descent(83/999): loss=13.542875252474953, w0=0.6387618139641948, w1=0.6387618139641948\n",
            "Gradient Descent(84/999): loss=13.507535334480545, w0=0.8009049240264148, w1=0.8009049240264148\n",
            "Gradient Descent(85/999): loss=13.473976779827632, w0=0.6429011886980375, w1=0.6429011886980375\n",
            "Gradient Descent(86/999): loss=13.442109796135973, w0=0.7968712239843958, w1=0.7968712239843958\n",
            "Gradient Descent(87/999): loss=13.411849117149412, w0=0.6468319118326107, w1=0.6468319118326107\n",
            "Gradient Descent(88/999): loss=13.383113774589614, w0=0.7930408488450149, w1=0.7930408488450149\n",
            "Gradient Descent(89/999): loss=13.355826881509788, w0=0.6505645007751745, w1=0.6505645007751745\n",
            "Gradient Descent(90/999): loss=13.32991542656888, w0=0.7894035497014117, w1=0.7894035497014117\n",
            "Gradient Descent(91/999): loss=13.305310078675642, w0=0.6541089427867746, w1=0.6541089427867746\n",
            "Gradient Descent(92/999): loss=13.281945001479947, w0=0.7859495942587614, w1=0.7859495942587614\n",
            "Gradient Descent(93/999): loss=13.259757677214912, w0=0.6574747217050838, w1=0.6574747217050838\n",
            "Gradient Descent(94/999): loss=13.238688739418636, w0=0.7826697407936448, w1=0.7826697407936448\n",
            "Gradient Descent(95/999): loss=13.218681814087706, w0=0.6606708433202377, w1=0.6606708433202377\n",
            "Gradient Descent(96/999): loss=13.199683368837754, w0=0.7795552134260355, w1=0.7795552134260355\n",
            "Gradient Descent(97/999): loss=13.181642569667193, w0=0.6637058594715617, w1=0.6637058594715617\n",
            "Gradient Descent(98/999): loss=13.16451114494111, w0=0.7765976786377418, w1=0.7765976786377418\n",
            "Gradient Descent(99/999): loss=13.14824325623114, w0=0.666587890929665, w1=0.666587890929665\n",
            "Gradient Descent(100/999): loss=13.132795375665953, w0=0.7737892229744734, w1=0.7737892229744734\n",
            "Gradient Descent(101/999): loss=13.11812616946405, w0=0.6693246491251273, w1=0.6693246491251273\n",
            "Gradient Descent(102/999): loss=13.104196387337236, w0=0.7711223318718685, w1=0.7711223318718685\n",
            "Gradient Descent(103/999): loss=13.090968757468946, w0=0.6719234567819181, w1=0.6719234567819181\n",
            "Gradient Descent(104/999): loss=13.078407886786293, w0=0.7685898695488296, w1=0.7685898695488296\n",
            "Gradient Descent(105/999): loss=13.066480166259069, w0=0.6743912675107552, w1=0.6743912675107552\n",
            "Gradient Descent(106/999): loss=13.055153680972362, w0=0.7661850599143648, w1=0.7661850599143648\n",
            "Gradient Descent(107/999): loss=13.044398124731968, w0=0.6767346844148302, w1=0.6767346844148302\n",
            "Gradient Descent(108/999): loss=13.034184718974418, w0=0.7639014684368518, w1=0.7639014684368518\n",
            "Gradient Descent(109/999): loss=13.02448613576431, w0=0.6789599777576841, w1=0.6789599777576841\n",
            "Gradient Descent(110/999): loss=13.015276424673194, w0=0.7617329849272081, w1=0.7617329849272081\n",
            "Gradient Descent(111/999): loss=13.00653094334418, w0=0.6810731017405054, w1=0.6810731017405054\n",
            "Gradient Descent(112/999): loss=12.998226291556595, w0=0.7596738071899017, w1=0.7596738071899017\n",
            "Gradient Descent(113/999): loss=12.990340248614121, w0=0.683079710433743, w1=0.683079710433743\n",
            "Gradient Descent(114/999): loss=12.982851713889113, w0=0.75771842549806, w1=0.75771842549806\n",
            "Gradient Descent(115/999): loss=12.975740650363754, w0=0.6849851729056615, w1=0.6849851729056615\n",
            "Gradient Descent(116/999): loss=12.968988031017156, w0=0.7558616078511331, w1=0.7558616078511331\n",
            "Gradient Descent(117/999): loss=12.962575787914924, w0=0.6867945875883177, w1=0.6867945875883177\n",
            "Gradient Descent(118/999): loss=12.956486763864891, w0=0.7540983859756677, w1=0.7540983859756677\n",
            "Gradient Descent(119/999): loss=12.950704666509766, w0=0.6885127959193985, w1=0.6885127959193985\n",
            "Gradient Descent(120/999): loss=12.94521402473377, w0=0.7524240420317349, w1=0.7524240420317349\n",
            "Gradient Descent(121/999): loss=12.94000014726669, w0=0.6901443952964198, w1=0.6901443952964198\n",
            "Gradient Descent(122/999): loss=12.935049083374574, w0=0.75083409598944, w1=0.75083409598944\n",
            "Gradient Descent(123/999): loss=12.930347585531827, w0=0.6916937513779503, w1=0.6916937513779503\n",
            "Gradient Descent(124/999): loss=12.925883073974905, w0=0.7493242936417386, w1=0.7493242936417386\n",
            "Gradient Descent(125/999): loss=12.921643603042773, w0=0.6931650097647718, w1=0.6931650097647718\n",
            "Gradient Descent(126/999): loss=12.917617829213928, w0=0.747890595221486, w1=0.747890595221486\n",
            "Gradient Descent(127/999): loss=12.91379498075466, w0=0.6945621070922336, w1=0.6945621070922336\n",
            "Gradient Descent(128/999): loss=12.910164828897193, w0=0.7465291645922626, w1=0.7465291645922626\n",
            "Gradient Descent(129/999): loss=12.906717660470642, w0=0.6958887815634784, w1=0.6958887815634784\n",
            "Gradient Descent(130/999): loss=12.903444251911564, w0=0.7452363589840499, w1=0.7452363589840499\n",
            "Gradient Descent(131/999): loss=12.900335844584507, w0=0.6971485829517254, w1=0.6971485829517254\n",
            "Gradient Descent(132/999): loss=12.897384121346644, w0=0.7440087192462992, w1=0.7440087192462992\n",
            "Gradient Descent(133/999): loss=12.894581184293592, w0=0.6983448820983729, w1=0.6983448820983729\n",
            "Gradient Descent(134/999): loss=12.891919533627107, w0=0.7428429605923057, w1=0.7428429605923057\n",
            "Gradient Descent(135/999): loss=12.88939204758788, w0=0.6994808799323335, w1=0.6994808799323335\n",
            "Gradient Descent(136/999): loss=12.886991963399929, w0=0.7417359638101306, w1=0.7417359638101306\n",
            "Gradient Descent(137/999): loss=12.884712859175476, w0=0.7005596160347374, w1=0.7005596160347374\n",
            "Gradient Descent(138/999): loss=12.882548636731942, w0=0.740684766916547, w1=0.740684766916547\n",
            "Gradient Descent(139/999): loss=12.880493505275107, w0=0.7015839767719161, w1=0.7015839767719161\n",
            "Gradient Descent(140/999): loss=12.878541965904699, w0=0.7396865572316838, w1=0.7396865572316838\n",
            "Gradient Descent(141/999): loss=12.876688796901039, w0=0.7025567030184328, w1=0.7025567030184328\n",
            "Gradient Descent(142/999): loss=12.874929039753322, w0=0.7387386638531578, w1=0.7387386638531578\n",
            "Gradient Descent(143/999): loss=12.873257985892163, w0=0.70348039749082, w1=0.70348039749082\n",
            "Gradient Descent(144/999): loss=12.871671164090921, w0=0.7378385505095606, w1=0.7378385505095606\n",
            "Gradient Descent(145/999): loss=12.870164328502057, w0=0.7043575317116494, w1=0.7043575317116494\n",
            "Gradient Descent(146/999): loss=12.86873344729657, w0=0.736983808774175, w1=0.736983808774175\n",
            "Gradient Descent(147/999): loss=12.867374691876034, w0=0.7051904526225666, w1=0.7051904526225666\n",
            "Gradient Descent(148/999): loss=12.86608442662846, w0=0.7361721516207657, w1=0.7361721516207657\n",
            "Gradient Descent(149/999): loss=12.864859199200538, w0=0.7059813888639861, w1=0.7059813888639861\n",
            "Gradient Descent(150/999): loss=12.86369573126016, w0=0.7354014073041992, w1=0.7354014073041992\n",
            "Gradient Descent(151/999): loss=12.862590909724636, w0=0.7067324567382478, w1=0.7067324567382478\n",
            "Gradient Descent(152/999): loss=12.861541778431011, w0=0.734669513549522, w1=0.734669513549522\n",
            "Gradient Descent(153/999): loss=12.860545530226347, w0=0.707445665872192, w1=0.707445665872192\n",
            "Gradient Descent(154/999): loss=12.859599499456564, w0=0.7339745120339456, w1=0.7339745120339456\n",
            "Gradient Descent(155/999): loss=12.858701154834035, w0=0.7081229245943038, w1=0.7081229245943038\n",
            "Gradient Descent(156/999): loss=12.85784809266459, w0=0.733314543146977, w1=0.733314543146977\n",
            "Gradient Descent(157/999): loss=12.857038030415996, w0=0.7087660450408126, w1=0.7087660450408126\n",
            "Gradient Descent(158/999): loss=12.856268800610593, w0=0.7326878410146728, w1=0.7326878410146728\n",
            "Gradient Descent(159/999): loss=12.855538345025801, w0=0.7093767480044126, w1=0.7093767480044126\n",
            "Gradient Descent(160/999): loss=12.85484470918693, w0=0.7320927287747022, w1=0.7320927287747022\n",
            "Gradient Descent(161/999): loss=12.854186037137614, w0=0.7099566675385754, w1=0.7099566675385754\n",
            "Gradient Descent(162/999): loss=12.853560566473847, w0=0.7315276140895796, w1=0.7315276140895796\n",
            "Gradient Descent(163/999): loss=12.852966623628337, w0=0.7105073553297745, w1=0.7105073553297745\n",
            "Gradient Descent(164/999): loss=12.85240261939253, w0=0.7309909848860582, w1=0.7309909848860582\n",
            "Gradient Descent(165/999): loss=12.851867044664408, w0=0.7110302848493213, w1=0.7110302848493213\n",
            "Gradient Descent(166/999): loss=12.85135846641057, w0=0.730481405309287, w1=0.730481405309287\n",
            "Gradient Descent(167/999): loss=12.85087552383188, w0=0.7115268552959225, w1=0.7115268552959225\n",
            "Gradient Descent(168/999): loss=12.850416924722412, w0=0.7299975118809041, w1=0.7299975118809041\n",
            "Gradient Descent(169/999): loss=12.849981442011858, w0=0.7119983953395052, w1=0.7119983953395052\n",
            "Gradient Descent(170/999): loss=12.849567910482305, w0=0.7295380098507878, w1=0.7295380098507878\n",
            "Gradient Descent(171/999): loss=12.849175223650448, w0=0.712446166676329, w1=0.712446166676329\n",
            "Gradient Descent(172/999): loss=12.848802330806995, w0=0.7291016697327032, w1=0.7291016697327032\n",
            "Gradient Descent(173/999): loss=12.848448234205286, w0=0.7128713674048986, w1=0.7128713674048986\n",
            "Gradient Descent(174/999): loss=12.848111986391613, w0=0.7286873240145745, w1=0.7286873240145745\n",
            "Gradient Descent(175/999): loss=12.847792687670154, w0=0.7132751352317062, w1=0.7132751352317062\n",
            "Gradient Descent(176/999): loss=12.847489483695645, w0=0.728293864034582, w1=0.728293864034582\n",
            "Gradient Descent(177/999): loss=12.847201563187422, w0=0.713658550515385, w1=0.713658550515385\n",
            "Gradient Descent(178/999): loss=12.846928155758683, w0=0.7279202370147229, w1=0.7279202370147229\n",
            "Gradient Descent(179/999): loss=12.84666852985515, w0=0.7140226391574166, w1=0.7140226391574166\n",
            "Gradient Descent(180/999): loss=12.84642199079769, w0=0.7275654432439016, w1=0.7275654432439016\n",
            "Gradient Descent(181/999): loss=12.846187878923548, w0=0.7143683753471288, w1=0.7143683753471288\n",
            "Gradient Descent(182/999): loss=12.845965567821285, w0=0.7272285334030101, w1=0.7272285334030101\n",
            "Gradient Descent(183/999): loss=12.845754462654696, w0=0.7146966841683258, w1=0.7146966841683258\n",
            "Gradient Descent(184/999): loss=12.84555399857123, w0=0.7269086060248411, w1=0.7269086060248411\n",
            "Gradient Descent(185/999): loss=12.8453636391906, w0=0.7150084440745291, w1=0.7150084440745291\n",
            "Gradient Descent(186/999): loss=12.8451828751696, w0=0.7266048050820391, w1=0.7266048050820391\n",
            "Gradient Descent(187/999): loss=12.84501122283926, w0=0.7153044892394491, w1=0.7153044892394491\n",
            "Gradient Descent(188/999): loss=12.84484822291067, w0=0.7263163176966342, w1=0.7263163176966342\n",
            "Gradient Descent(189/999): loss=12.844693439246127, w0=0.7155856117889771, w1=0.7155856117889771\n",
            "Gradient Descent(190/999): loss=12.84454645769211, w0=0.7260423719650307, w1=0.7260423719650307\n",
            "Gradient Descent(191/999): loss=12.844406884971123, w0=0.7158525639206711, w1=0.7158525639206711\n",
            "Gradient Descent(192/999): loss=12.84427434762947, w0=0.72578223489263, w1=0.72578223489263\n",
            "Gradient Descent(193/999): loss=12.844148491037958, w0=0.7161060599164055, w1=0.7161060599164055\n",
            "Gradient Descent(194/999): loss=12.844028978443008, w0=0.7255352104325623, w1=0.7255352104325623\n",
            "Gradient Descent(195/999): loss=12.843915490065644, w0=0.7163467780535696, w1=0.7163467780535696\n",
            "Gradient Descent(196/999): loss=12.84380772224584, w0=0.725300637623279, w1=0.725300637623279\n",
            "Gradient Descent(197/999): loss=12.843705386630031, w0=0.7165753624199297, w1=0.7165753624199297\n",
            "Gradient Descent(198/999): loss=12.843608209399548, w0=0.7250778888200231, w1=0.7250778888200231\n",
            "Gradient Descent(199/999): loss=12.843515930537992, w0=0.7167924246370099, w1=0.7167924246370099\n",
            "Gradient Descent(200/999): loss=12.843428303135472, w0=0.7248663680154439, w1=0.7248663680154439\n",
            "Gradient Descent(201/999): loss=12.843345092728, w0=0.7169985454966032, w1=0.7169985454966032\n",
            "Gradient Descent(202/999): loss=12.843266076670087, w0=0.7246655092448656, w1=0.7246655092448656\n",
            "Gradient Descent(203/999): loss=12.843191043539042, w0=0.7171942765147925, w1=0.7171942765147925\n",
            "Gradient Descent(204/999): loss=12.843119792569267, w0=0.7244747750719386, w1=0.7244747750719386\n",
            "Gradient Descent(205/999): loss=12.84305213311506, w0=0.7173801414076373, w1=0.7173801414076373\n",
            "Gradient Descent(206/999): loss=12.842987884140504, w0=0.7242936551506258, w1=0.7242936551506258\n",
            "Gradient Descent(207/999): loss=12.842926873735104, w0=0.7175566374924774, w1=0.7175566374924774\n",
            "Gradient Descent(208/999): loss=12.842868938653744, w0=0.7241216648596731, w1=0.7241216648596731\n",
            "Gradient Descent(209/999): loss=12.842813923879952, w0=0.7177242370186018, w1=0.7177242370186018\n",
            "Gradient Descent(210/999): loss=12.842761682211108, w0=0.7239583440059114, w1=0.7239583440059114\n",
            "Gradient Descent(211/999): loss=12.842712073864558, w0=0.7178833884308415, w1=0.7178833884308415\n",
            "Gradient Descent(212/999): loss=12.842664966103602, w0=0.7238032555929224, w1=0.7238032555929224\n",
            "Gradient Descent(213/999): loss=12.84262023288235, w0=0.7180345175694711, w1=0.7180345175694711\n",
            "Gradient Descent(214/999): loss=12.842577754508453, w0=0.7236559846517687, w1=0.7236559846517687\n",
            "Gradient Descent(215/999): loss=12.842537417322827, w0=0.7181780288096261, w1=0.7181780288096261\n",
            "Gradient Descent(216/999): loss=12.842499113395563, w0=0.723516137130666, w1=0.723516137130666\n",
            "Gradient Descent(217/999): loss=12.84246274023713, w0=0.7183143061432863, w1=0.7183143061432863\n",
            "Gradient Descent(218/999): loss=12.842428200524116, w0=0.7233833388406207, w1=0.7233833388406207\n",
            "Gradient Descent(219/999): loss=12.842395401838875, w0=0.7184437142067206, w1=0.7184437142067206\n",
            "Gradient Descent(220/999): loss=12.842364256422197, w0=0.7232572344542154, w1=0.7232572344542154\n",
            "Gradient Descent(221/999): loss=12.842334680938523, w0=0.7185665992561419, w1=0.7185665992561419\n",
            "Gradient Descent(222/999): loss=12.842306596252941, w0=0.7231374865548621, w1=0.7231374865548621\n",
            "Gradient Descent(223/999): loss=12.842279927219469, w0=0.7186832900941819, w1=0.7186832900941819\n",
            "Gradient Descent(224/999): loss=12.842254602479965, w0=0.7230237747339784, w1=0.7230237747339784\n",
            "Gradient Descent(225/999): loss=12.842230554273202, w0=0.7187940989496658, w1=0.7187940989496658\n",
            "Gradient Descent(226/999): loss=12.842207718253576, w0=0.7229157947336736, w1=0.7229157947336736\n",
            "Gradient Descent(227/999): loss=12.842186033318923, w0=0.7188993223130413, w1=0.7188993223130413\n",
            "Gradient Descent(228/999): loss=12.842165441447001, w0=0.7228132576326476, w1=0.7228132576326476\n",
            "Gradient Descent(229/999): loss=12.842145887540301, w0=0.7189992417296951, w1=0.7189992417296951\n",
            "Gradient Descent(230/999): loss=12.842127319278584, w0=0.7227158890731258, w1=0.7227158890731258\n",
            "Gradient Descent(231/999): loss=12.842109686978873, w0=0.7190941245532828, w1=0.7190941245532828\n",
            "Gradient Descent(232/999): loss=12.842092943462553, w0=0.7226234285267628, w1=0.7226234285267628\n",
            "Gradient Descent(233/999): loss=12.842077043929127, w0=0.7191842246610842, w1=0.7191842246610842\n",
            "Gradient Descent(234/999): loss=12.842061945836328, w0=0.7225356285975479, w1=0.7225356285975479\n",
            "Gradient Descent(235/999): loss=12.842047608786302, w0=0.7192697831333017, w1=0.7192697831333017\n",
            "Gradient Descent(236/999): loss=12.842033994417498, w0=0.7224522543598505, w1=0.7224522543598505\n",
            "Gradient Descent(237/999): loss=12.84202106630206, w0=0.7193510288981159, w1=0.7193510288981159\n",
            "Gradient Descent(238/999): loss=12.842008789848318, w0=0.7223730827298298, w1=0.7223730827298298\n",
            "Gradient Descent(239/999): loss=12.841997132208249, w0=0.719428179344228, w1=0.719428179344228\n",
            "Gradient Descent(240/999): loss=12.841986062189605, w0=0.722297901868532, w1=0.722297901868532\n",
            "Gradient Descent(241/999): loss=12.841975550172405, w0=0.7195014409025248, w1=0.7195014409025248\n",
            "Gradient Descent(242/999): loss=12.841965568029721, w0=0.7222265106150744, w1=0.7222265106150744\n",
            "Gradient Descent(243/999): loss=12.841956089052413, w0=0.719571009598424, w1=0.719571009598424\n",
            "Gradient Descent(244/999): loss=12.841947087877651, w0=0.7221587179484, w1=0.7221587179484\n",
            "Gradient Descent(245/999): loss=12.841938540421053, w0=0.7196370715763788, w1=0.7196370715763788\n",
            "Gradient Descent(246/999): loss=12.841930423812256, w0=0.7220943424761641, w1=0.7220943424761641\n",
            "Gradient Descent(247/999): loss=12.841922716333704, w0=0.7196998035979423, w1=0.7196998035979423\n",
            "Gradient Descent(248/999): loss=12.841915397362548, w0=0.7220332119493849, w1=0.7220332119493849\n",
            "Gradient Descent(249/999): loss=12.841908447315486, w0=0.7197593735147275, w1=0.7197593735147275\n",
            "Gradient Descent(250/999): loss=12.841901847596311, w0=0.721975162801557, w1=0.721975162801557\n",
            "Gradient Descent(251/999): loss=12.84189558054621, w0=0.719815940717527, w1=0.719815940717527\n",
            "Gradient Descent(252/999): loss=12.84188962939649, w0=0.7219200397109986, w1=0.7219200397109986\n",
            "Gradient Descent(253/999): loss=12.841883978223692, w0=0.7198696565627931, w1=0.7198696565627931\n",
            "Gradient Descent(254/999): loss=12.841878611907017, w0=0.7218676951852576, w1=0.7218676951852576\n",
            "Gradient Descent(255/999): loss=12.841873516087853, w0=0.719920664777622, w1=0.719920664777622\n",
            "Gradient Descent(256/999): loss=12.841868677131352, w0=0.7218179891664678, w1=0.7218179891664678\n",
            "Gradient Descent(257/999): loss=12.841864082089957, w0=0.7199691018443224, w1=0.7199691018443224\n",
            "Gradient Descent(258/999): loss=12.841859718668736, w0=0.7217707886565968, w1=0.7217707886565968\n",
            "Gradient Descent(259/999): loss=12.841855575192533, w0=0.7200150973656008, w1=0.7200150973656008\n",
            "Gradient Descent(260/999): loss=12.841851640574669, w0=0.7217259673615841, w1=0.7217259673615841\n",
            "Gradient Descent(261/999): loss=12.841847904287318, w0=0.7200587744113387, w1=0.7200587744113387\n",
            "Gradient Descent(262/999): loss=12.84184435633332, w0=0.721683405353417, w1=0.721683405353417\n",
            "Gradient Descent(263/999): loss=12.841840987219435, w0=0.7201002498478891, w1=0.7201002498478891\n",
            "Gradient Descent(264/999): loss=12.841837787930972, w0=0.7216429887492405, w1=0.7216429887492405\n",
            "Gradient Descent(265/999): loss=12.841834749907594, w0=0.7201396346507755, w1=0.7201396346507755\n",
            "Gradient Descent(266/999): loss=12.84183186502049, w0=0.7216046094066411, w1=0.7216046094066411\n",
            "Gradient Descent(267/999): loss=12.841829125550598, w0=0.7201770342016279, w1=0.7201770342016279\n",
            "Gradient Descent(268/999): loss=12.84182652416793, w0=0.7215681646342914, w1=0.7215681646342914\n",
            "Gradient Descent(269/999): loss=12.84182405391199, w0=0.7202125485701524, w1=0.7202125485701524\n",
            "Gradient Descent(270/999): loss=12.841821708173125, w0=0.7215335569171794, w1=0.7215335569171794\n",
            "Gradient Descent(271/999): loss=12.841819480674861, w0=0.7202462727818859, w1=0.7202462727818859\n",
            "Gradient Descent(272/999): loss=12.841817365457107, w0=0.7215006936556886, w1=0.7215006936556886\n",
            "Gradient Descent(273/999): loss=12.841815356860181, w0=0.7202782970724564, w1=0.7202782970724564\n",
            "Gradient Descent(274/999): loss=12.841813449509692, w0=0.7214694869178295, w1=0.7214694869178295\n",
            "Gradient Descent(275/999): loss=12.841811638302163, w0=0.7203087071290246, w1=0.7203087071290246\n",
            "Gradient Descent(276/999): loss=12.841809918391355, w0=0.721439853203961, w1=0.721439853203961\n",
            "Gradient Descent(277/999): loss=12.841808285175318, w0=0.7203375843195576, w1=0.7203375843195576\n",
            "Gradient Descent(278/999): loss=12.84180673428407, w0=0.7214117132233707, w1=0.7214117132233707\n",
            "Gradient Descent(279/999): loss=12.841805261567897, w0=0.7203650059105436, w1=0.7203650059105436\n",
            "Gradient Descent(280/999): loss=12.84180386308626, w0=0.7213849916821176, w1=0.7213849916821176\n",
            "Gradient Descent(281/999): loss=12.841802535097267, w0=0.720391045273734, w1=0.720391045273734\n",
            "Gradient Descent(282/999): loss=12.841801274047612, w0=0.7213596170815683, w1=0.7213596170815683\n",
            "Gradient Descent(283/999): loss=12.841800076563121, w0=0.7204157720824632, w1=0.7204157720824632\n",
            "Gradient Descent(284/999): loss=12.841798939439691, w0=0.7213355215270889, w1=0.7213355215270889\n",
            "Gradient Descent(285/999): loss=12.841797859634728, w0=0.7204392524980731, w1=0.7204392524980731\n",
            "Gradient Descent(286/999): loss=12.841796834259009, w0=0.7213126405463799, w1=0.7213126405463799\n",
            "Gradient Descent(287/999): loss=12.84179586056895, w0=0.720461549346941, w1=0.720461549346941\n",
            "Gradient Descent(288/999): loss=12.84179493595924, w0=0.7212909129169677, w1=0.7212909129169677\n",
            "Gradient Descent(289/999): loss=12.841794057955914, w0=0.7204827222885826, w1=0.7204827222885826\n",
            "Gradient Descent(290/999): loss=12.841793224209711, w0=0.7212702805023927, w1=0.7212702805023927\n",
            "Gradient Descent(291/999): loss=12.84179243248977, w0=0.7205028279752839, w1=0.7205028279752839\n",
            "Gradient Descent(292/999): loss=12.84179168067771, w0=0.7212506880966536, w1=0.7212506880966536\n",
            "Gradient Descent(293/999): loss=12.8417909667619, w0=0.7205219202036843, w1=0.7205219202036843\n",
            "Gradient Descent(294/999): loss=12.841790288832117, w0=0.7212320832764929, w1=0.7212320832764929\n",
            "Gradient Descent(295/999): loss=12.841789645074437, w0=0.7205400500587201, w1=0.7205400500587201\n",
            "Gradient Descent(296/999): loss=12.841789033766368, w0=0.7212144162611289, w1=0.7212144162611289\n",
            "Gradient Descent(297/999): loss=12.84178845327222, w0=0.7205572660503127, w1=0.7205572660503127\n",
            "Gradient Descent(298/999): loss=12.841787902038789, w0=0.7211976397790574, w1=0.7211976397790574\n",
            "Gradient Descent(299/999): loss=12.841787378591123, w0=0.720573614243165, w1=0.720573614243165\n",
            "Gradient Descent(300/999): loss=12.841786881528655, w0=0.7211817089415673, w1=0.7211817089415673\n",
            "Gradient Descent(301/999): loss=12.841786409521387, w0=0.7205891383800178, w1=0.7205891383800178\n",
            "Gradient Descent(302/999): loss=12.841785961306382, w0=0.7211665811226328, w1=0.7211665811226328\n",
            "Gradient Descent(303/999): loss=12.841785535684345, w0=0.720603879998691, w1=0.720603879998691\n",
            "Gradient Descent(304/999): loss=12.841785131516449, w0=0.7211522158448589, w1=0.7211522158448589\n",
            "Gradient Descent(305/999): loss=12.841784747721258, w0=0.7206178785432263, w1=0.7206178785432263\n",
            "Gradient Descent(306/999): loss=12.841784383271865, w0=0.7211385746711767, w1=0.7211385746711767\n",
            "Gradient Descent(307/999): loss=12.841784037193104, w0=0.7206311714694273, w1=0.7206311714694273\n",
            "Gradient Descent(308/999): loss=12.841783708558978, w0=0.721125621101997, w1=0.721125621101997\n",
            "Gradient Descent(309/999): loss=12.841783396490168, w0=0.7206437943450797, w1=0.7206437943450797\n",
            "Gradient Descent(310/999): loss=12.841783100151668, w0=0.7211133204775493, w1=0.7211133204775493\n",
            "Gradient Descent(311/999): loss=12.84178281875057, w0=0.7206557809451202, w1=0.7206557809451202\n",
            "Gradient Descent(312/999): loss=12.841782551533926, w0=0.721101639885142, w1=0.721101639885142\n",
            "Gradient Descent(313/999): loss=12.841782297786757, w0=0.7206671633420071, w1=0.7206671633420071\n",
            "Gradient Descent(314/999): loss=12.841782056830107, w0=0.7210905480710993, w1=0.7210905480710993\n",
            "Gradient Descent(315/999): loss=12.841781828019247, w0=0.7206779719915367, w1=0.7206779719915367\n",
            "Gradient Descent(316/999): loss=12.841781610741961, w0=0.7210800153571352, w1=0.7210800153571352\n",
            "Gradient Descent(317/999): loss=12.841781404416867, w0=0.7206882358143339, w1=0.7206882358143339\n",
            "Gradient Descent(318/999): loss=12.84178120849191, w0=0.7210700135609436, w1=0.7210700135609436\n",
            "Gradient Descent(319/999): loss=12.841781022442852, w0=0.7206979822732347, w1=0.7206979822732347\n",
            "Gradient Descent(320/999): loss=12.841780845771886, w0=0.7210605159207913, w1=0.7210605159207913\n",
            "Gradient Descent(321/999): loss=12.841780678006296, w0=0.720707237446768, w1=0.720707237446768\n",
            "Gradient Descent(322/999): loss=12.84178051869719, w0=0.7210514970239118, w1=0.7210514970239118\n",
            "Gradient Descent(323/999): loss=12.841780367418304, w0=0.7207160260989344, w1=0.7207160260989344\n",
            "Gradient Descent(324/999): loss=12.841780223764873, w0=0.7210429327385084, w1=0.7210429327385084\n",
            "Gradient Descent(325/999): loss=12.841780087352507, w0=0.7207243717454662, w1=0.7207243717454662\n",
            "Gradient Descent(326/999): loss=12.84177995781622, w0=0.7210348001491849, w1=0.7210348001491849\n",
            "Gradient Descent(327/999): loss=12.841779834809415, w0=0.7207322967167494, w1=0.7207322967167494\n",
            "Gradient Descent(328/999): loss=12.841779718002948, w0=0.7210270774956312, w1=0.7210270774956312\n",
            "Gradient Descent(329/999): loss=12.841779607084302, w0=0.7207398222175719, w1=0.7207398222175719\n",
            "Gradient Descent(330/999): loss=12.841779501756676, w0=0.7210197441143995, w1=0.7210197441143995\n",
            "Gradient Descent(331/999): loss=12.841779401738254, w0=0.7207469683838613, w1=0.7207469683838613\n",
            "Gradient Descent(332/999): loss=12.84177930676142, w0=0.7210127803836153, w1=0.7210127803836153\n",
            "Gradient Descent(333/999): loss=12.841779216572037, w0=0.7207537543365627, w1=0.7207537543365627\n",
            "Gradient Descent(334/999): loss=12.841779130928787, w0=0.7210061676704745, w1=0.7210061676704745\n",
            "Gradient Descent(335/999): loss=12.841779049602518, w0=0.7207601982328004, w1=0.7207601982328004\n",
            "Gradient Descent(336/999): loss=12.841778972375632, w0=0.720999888281389, w1=0.720999888281389\n",
            "Gradient Descent(337/999): loss=12.841778899041486, w0=0.7207663173144604, w1=0.7207663173144604\n",
            "Gradient Descent(338/999): loss=12.841778829403857, w0=0.7209939254146432, w1=0.7209939254146432\n",
            "Gradient Descent(339/999): loss=12.841778763276423, w0=0.7207721279543251, w1=0.7207721279543251\n",
            "Gradient Descent(340/999): loss=12.841778700482239, w0=0.7209882631154377, w1=0.7209882631154377\n",
            "Gradient Descent(341/999): loss=12.841778640853304, w0=0.7207776456998812, w1=0.7207776456998812\n",
            "Gradient Descent(342/999): loss=12.841778584230049, w0=0.7209828862332001, w1=0.7209828862332001\n",
            "Gradient Descent(343/999): loss=12.841778530460973, w0=0.7207828853149204, w1=0.7207828853149204\n",
            "Gradient Descent(344/999): loss=12.841778479402219, w0=0.7209777803810453, w1=0.7209777803810453\n",
            "Gradient Descent(345/999): loss=12.841778430917167, w0=0.7207878608190424, w1=0.7207878608190424\n",
            "Gradient Descent(346/999): loss=12.841778384876067, w0=0.7209729318972824, w1=0.7209729318972824\n",
            "Gradient Descent(347/999): loss=12.841778341155743, w0=0.7207925855251673, w1=0.7207925855251673\n",
            "Gradient Descent(348/999): loss=12.84177829963921, w0=0.7209683278088583, w1=0.7209683278088583\n",
            "Gradient Descent(349/999): loss=12.841778260215387, w0=0.7207970720751571, w1=0.7207970720751571\n",
            "Gradient Descent(350/999): loss=12.841778222778775, w0=0.7209639557966474, w1=0.7209639557966474\n",
            "Gradient Descent(351/999): loss=12.841778187229217, w0=0.7208013324736406, w1=0.7208013324736406\n",
            "Gradient Descent(352/999): loss=12.84177815347159, w0=0.7209598041624884, w1=0.7209598041624884\n",
            "Gradient Descent(353/999): loss=12.84177812141557, w0=0.7208053781201352, w1=0.7208053781201352\n",
            "Gradient Descent(354/999): loss=12.841778090975382, w0=0.7209558617978838, w1=0.7209558617978838\n",
            "Gradient Descent(355/999): loss=12.841778062069578, w0=0.7208092198395478, w1=0.7208092198395478\n",
            "Gradient Descent(356/999): loss=12.841778034620821, w0=0.7209521181542778, w1=0.7209521181542778\n",
            "Gradient Descent(357/999): loss=12.841778008555654, w0=0.7208128679111387, w1=0.7208128679111387\n",
            "Gradient Descent(358/999): loss=12.841777983804345, w0=0.7209485632148301, w1=0.7209485632148301\n",
            "Gradient Descent(359/999): loss=12.841777960300663, w0=0.720816332096027, w1=0.720816332096027\n",
            "Gradient Descent(360/999): loss=12.841777937981718, w0=0.7209451874676153, w1=0.7209451874676153\n",
            "Gradient Descent(361/999): loss=12.841777916787798, w0=0.7208196216633066, w1=0.7208196216633066\n",
            "Gradient Descent(362/999): loss=12.841777896662192, w0=0.720941981880171, w1=0.720941981880171\n",
            "Gradient Descent(363/999): loss=12.841777877551046, w0=0.7208227454148487, w1=0.7208227454148487\n",
            "Gradient Descent(364/999): loss=12.841777859403225, w0=0.7209389378753304, w1=0.7209389378753304\n",
            "Gradient Descent(365/999): loss=12.841777842170176, w0=0.7208257117088528, w1=0.7208257117088528\n",
            "Gradient Descent(366/999): loss=12.841777825805789, w0=0.7209360473082718, w1=0.7209360473082718\n",
            "Gradient Descent(367/999): loss=12.841777810266267, w0=0.72082852848221, w1=0.72082852848221\n",
            "Gradient Descent(368/999): loss=12.841777795510046, w0=0.720933302444726, w1=0.720933302444726\n",
            "Gradient Descent(369/999): loss=12.841777781497624, w0=0.7208312032717403, w1=0.7208312032717403\n",
            "Gradient Descent(370/999): loss=12.84177776819153, w0=0.7209306959402815, w1=0.7209306959402815\n",
            "Gradient Descent(371/999): loss=12.841777755556148, w0=0.7208337432343587, w1=0.7208337432343587\n",
            "Gradient Descent(372/999): loss=12.841777743557667, w0=0.7209282208207336, w1=0.7209282208207336\n",
            "Gradient Descent(373/999): loss=12.841777732164, w0=0.7208361551662242, w1=0.7208361551662242\n",
            "Gradient Descent(374/999): loss=12.841777721344634, w0=0.7209258704634228, w1=0.7209258704634228\n",
            "Gradient Descent(375/999): loss=12.841777711070641, w0=0.7208384455209259, w1=0.7208384455209259\n",
            "Gradient Descent(376/999): loss=12.841777701314525, w0=0.7209236385795152, w1=0.7209236385795152\n",
            "Gradient Descent(377/999): loss=12.841777692050183, w0=0.7208406204267487, w1=0.7208406204267487\n",
            "Gradient Descent(378/999): loss=12.841777683252824, w0=0.7209215191971753, w1=0.7209215191971753\n",
            "Gradient Descent(379/999): loss=12.841777674898907, w0=0.7208426857030727, w1=0.7208426857030727\n",
            "Gradient Descent(380/999): loss=12.841777666966085, w0=0.7209195066455871, w1=0.7209195066455871\n",
            "Gradient Descent(381/999): loss=12.841777659433124, w0=0.7208446468759424, w1=0.7208446468759424\n",
            "Gradient Descent(382/999): loss=12.841777652279879, w0=0.7209175955397811, w1=0.7209175955397811\n",
            "Gradient Descent(383/999): loss=12.841777645487205, w0=0.720846509192854, w1=0.720846509192854\n",
            "Gradient Descent(384/999): loss=12.84177763903692, w0=0.7209157807662253, w1=0.7209157807662253\n",
            "Gradient Descent(385/999): loss=12.84177763291178, w0=0.7208482776367952, w1=0.7208482776367952\n",
            "Gradient Descent(386/999): loss=12.841777627095382, w0=0.7209140574691434, w1=0.7209140574691434\n",
            "Gradient Descent(387/999): loss=12.841777621572168, w0=0.7208499569395785, w1=0.7208499569395785\n",
            "Gradient Descent(388/999): loss=12.841777616327361, w0=0.7209124210375218, w1=0.7209124210375218\n",
            "Gradient Descent(389/999): loss=12.841777611346924, w0=0.7208515515945022, w1=0.7208515515945022\n",
            "Gradient Descent(390/999): loss=12.841777606617539, w0=0.7209108670927726, w1=0.7209108670927726\n",
            "Gradient Descent(391/999): loss=12.841777602126541, w0=0.7208530658683729, w1=0.7208530658683729\n",
            "Gradient Descent(392/999): loss=12.841777597861922, w0=0.7209093914770169, w1=0.7209093914770169\n",
            "Gradient Descent(393/999): loss=12.84177759381227, w0=0.720854503812922, w1=0.720854503812922\n",
            "Gradient Descent(394/999): loss=12.841777589966743, w0=0.7209079902419607, w1=0.7209079902419607\n",
            "Gradient Descent(395/999): loss=12.841777586315052, w0=0.7208558692756474, w1=0.7208558692756474\n",
            "Gradient Descent(396/999): loss=12.841777582847442, w0=0.7209066596383293, w1=0.7209066596383293\n",
            "Gradient Descent(397/999): loss=12.841777579554615, w0=0.7208571659101076, w1=0.7208571659101076\n",
            "Gradient Descent(398/999): loss=12.84177757642777, w0=0.7209053961058363, w1=0.7209053961058363\n",
            "Gradient Descent(399/999): loss=12.84177757345853, w0=0.7208583971856983, w1=0.7208583971856983\n",
            "Gradient Descent(400/999): loss=12.841777570638964, w0=0.720904196263657, w1=0.720904196263657\n",
            "Gradient Descent(401/999): loss=12.841777567961525, w0=0.7208595663969343, w1=0.7208595663969343\n",
            "Gradient Descent(402/999): loss=12.841777565419049, w0=0.7209030569013825, w1=0.7209030569013825\n",
            "Gradient Descent(403/999): loss=12.841777563004728, w0=0.7208606766722658, w1=0.7208606766722658\n",
            "Gradient Descent(404/999): loss=12.841777560712105, w0=0.7209019749704292, w1=0.7209019749704292\n",
            "Gradient Descent(405/999): loss=12.841777558535046, w0=0.7208617309824484, w1=0.7208617309824484\n",
            "Gradient Descent(406/999): loss=12.841777556467717, w0=0.7209009475758826, w1=0.7209009475758826\n",
            "Gradient Descent(407/999): loss=12.841777554504606, w0=0.7208627321484922, w1=0.7208627321484922\n",
            "Gradient Descent(408/999): loss=12.841777552640439, w0=0.7208999719687506, w1=0.7208999719687506\n",
            "Gradient Descent(409/999): loss=12.841777550870244, w0=0.7208636828492097, w1=0.7208636828492097\n",
            "Gradient Descent(410/999): loss=12.841777549189276, w0=0.7208990455386082, w1=0.7208990455386082\n",
            "Gradient Descent(411/999): loss=12.841777547593042, w0=0.720864585628384, w1=0.720864585628384\n",
            "Gradient Descent(412/999): loss=12.841777546077271, w0=0.720898165806613, w1=0.720898165806613\n",
            "Gradient Descent(413/999): loss=12.841777544637898, w0=0.7208654429015747, w1=0.7208654429015747\n",
            "Gradient Descent(414/999): loss=12.841777543271085, w0=0.7208973304188727, w1=0.7208973304188727\n",
            "Gradient Descent(415/999): loss=12.841777541973164, w0=0.7208662569625814, w1=0.7208662569625814\n",
            "Gradient Descent(416/999): loss=12.841777540740667, w0=0.7208965371401461, w1=0.7208965371401461\n",
            "Gradient Descent(417/999): loss=12.8417775395703, w0=0.7208670299895815, w1=0.7208670299895815\n",
            "Gradient Descent(418/999): loss=12.841777538458928, w0=0.7208957838478635, w1=0.7208957838478635\n",
            "Gradient Descent(419/999): loss=12.841777537403567, w0=0.7208677640509574, w1=0.7208677640509574\n",
            "Gradient Descent(420/999): loss=12.841777536401414, w0=0.7208950685264457, w1=0.7208950685264457\n",
            "Gradient Descent(421/999): loss=12.841777535449769, w0=0.720868461110832, w1=0.720868461110832\n",
            "Gradient Descent(422/999): loss=12.841777534546093, w0=0.7208943892619127, w1=0.7208943892619127\n",
            "Gradient Descent(423/999): loss=12.841777533687969, w0=0.7208691230343232, w1=0.7208691230343232\n",
            "Gradient Descent(424/999): loss=12.841777532873103, w0=0.7208937442367614, w1=0.7208937442367614\n",
            "Gradient Descent(425/999): loss=12.841777532099313, w0=0.7208697515925347, w1=0.7208697515925347\n",
            "Gradient Descent(426/999): loss=12.841777531364524, w0=0.7208931317251027, w1=0.7208931317251027\n",
            "Gradient Descent(427/999): loss=12.841777530666773, w0=0.7208703484672955, w1=0.7208703484672955\n",
            "Gradient Descent(428/999): loss=12.841777530004192, w0=0.7208925500880433, w1=0.7208925500880433\n",
            "Gradient Descent(429/999): loss=12.841777529375014, w0=0.7208709152556588, w1=0.7208709152556588\n",
            "Gradient Descent(430/999): loss=12.841777528777545, w0=0.7208919977693016, w1=0.7208919977693016\n",
            "Gradient Descent(431/999): loss=12.841777528210198, w0=0.7208714534741761, w1=0.7208714534741761\n",
            "Gradient Descent(432/999): loss=12.841777527671443, w0=0.7208914732910421, w1=0.7208914732910421\n",
            "Gradient Descent(433/999): loss=12.841777527159849, w0=0.7208719645629549, w1=0.7208719645629549\n",
            "Gradient Descent(434/999): loss=12.841777526674042, w0=0.7208909752499221, w1=0.7208909752499221\n",
            "Gradient Descent(435/999): loss=12.84177752621272, w0=0.7208724498895116, w1=0.7208724498895116\n",
            "Gradient Descent(436/999): loss=12.841777525774651, w0=0.7208905023133366, w1=0.7208905023133366\n",
            "Gradient Descent(437/999): loss=12.841777525358669, w0=0.7208729107524311, w1=0.7208729107524311\n",
            "Gradient Descent(438/999): loss=12.841777524963657, w0=0.7208900532158529, w1=0.7208900532158529\n",
            "Gradient Descent(439/999): loss=12.841777524588545, w0=0.7208733483848406, w1=0.7208733483848406\n",
            "Gradient Descent(440/999): loss=12.841777524232354, w0=0.7208896267558239, w1=0.7208896267558239\n",
            "Gradient Descent(441/999): loss=12.841777523894109, w0=0.7208737639577103, w1=0.7208737639577103\n",
            "Gradient Descent(442/999): loss=12.841777523572915, w0=0.7208892217921736, w1=0.7208892217921736\n",
            "Gradient Descent(443/999): loss=12.841777523267915, w0=0.7208741585829854, w1=0.7208741585829854\n",
            "Gradient Descent(444/999): loss=12.841777522978285, w0=0.7208888372413439, w1=0.7208888372413439\n",
            "Gradient Descent(445/999): loss=12.841777522703257, w0=0.7208745333165619, w1=0.7208745333165619\n",
            "Gradient Descent(446/999): loss=12.84177752244209, w0=0.7208884720743945, w1=0.7208884720743945\n",
            "Gradient Descent(447/999): loss=12.84177752219409, w0=0.7208748891611118, w1=0.7208748891611118\n",
            "Gradient Descent(448/999): loss=12.841777521958587, w0=0.7208881253142514, w1=0.7208881253142514\n",
            "Gradient Descent(449/999): loss=12.841777521734961, w0=0.7208752270687654, w1=0.7208752270687654\n",
            "Gradient Descent(450/999): loss=12.841777521522602, w0=0.7208877960330908, w1=0.7208877960330908\n",
            "Gradient Descent(451/999): loss=12.841777521320951, w0=0.7208755479436599, w1=0.7208755479436599\n",
            "Gradient Descent(452/999): loss=12.84177752112946, w0=0.7208874833498575, w1=0.7208874833498575\n",
            "Gradient Descent(453/999): loss=12.841777520947621, w0=0.7208758526443582, w1=0.7208758526443582\n",
            "Gradient Descent(454/999): loss=12.841777520774954, w0=0.7208871864279072, w1=0.7208871864279072\n",
            "Gradient Descent(455/999): loss=12.841777520610984, w0=0.7208761419861455, w1=0.7208761419861455\n",
            "Gradient Descent(456/999): loss=12.841777520455281, w0=0.7208869044727679, w1=0.7208869044727679\n",
            "Gradient Descent(457/999): loss=12.841777520307431, w0=0.7208764167432115, w1=0.7208764167432115\n",
            "Gradient Descent(458/999): loss=12.841777520167028, w0=0.7208866367300145, w1=0.7208866367300145\n",
            "Gradient Descent(459/999): loss=12.841777520033707, w0=0.7208766776507216, w1=0.7208766776507216\n",
            "Gradient Descent(460/999): loss=12.841777519907101, w0=0.7208863824832495, w1=0.7208863824832495\n",
            "Gradient Descent(461/999): loss=12.841777519786884, w0=0.7208769254067845, w1=0.7208769254067845\n",
            "Gradient Descent(462/999): loss=12.841777519672721, w0=0.7208861410521867, w1=0.7208861410521867\n",
            "Gradient Descent(463/999): loss=12.841777519564314, w0=0.720877160674319, w1=0.720877160674319\n",
            "Gradient Descent(464/999): loss=12.84177751946137, w0=0.720885911790831, w1=0.720885911790831\n",
            "Gradient Descent(465/999): loss=12.841777519363617, w0=0.7208773840828286, w1=0.7208773840828286\n",
            "Gradient Descent(466/999): loss=12.84177751927079, w0=0.7208856940857491, w1=0.7208856940857491\n",
            "Gradient Descent(467/999): loss=12.84177751918264, w0=0.7208775962300863, w1=0.7208775962300863\n",
            "Gradient Descent(468/999): loss=12.841777519098937, w0=0.7208854873544294, w1=0.7208854873544294\n",
            "Gradient Descent(469/999): loss=12.841777519019454, w0=0.7208777976837325, w1=0.7208777976837325\n",
            "Gradient Descent(470/999): loss=12.841777518943978, w0=0.7208852910437222, w1=0.7208852910437222\n",
            "Gradient Descent(471/999): loss=12.8417775188723, w0=0.7208779889827953, w1=0.7208779889827953\n",
            "Gradient Descent(472/999): loss=12.841777518804241, w0=0.7208851046283606, w1=0.7208851046283606\n",
            "Gradient Descent(473/999): loss=12.84177751873961, w0=0.7208781706391324, w1=0.7208781706391324\n",
            "Gradient Descent(474/999): loss=12.84177751867824, w0=0.7208849276095542, w1=0.7208849276095542\n",
            "Gradient Descent(475/999): loss=12.841777518619963, w0=0.7208783431388004, w1=0.7208783431388004\n",
            "Gradient Descent(476/999): loss=12.841777518564614, w0=0.720884759513655, w1=0.720884759513655\n",
            "Gradient Descent(477/999): loss=12.841777518512066, w0=0.7208785069433551, w1=0.7208785069433551\n",
            "Gradient Descent(478/999): loss=12.841777518462163, w0=0.7208845998908904, w1=0.7208845998908904\n",
            "Gradient Descent(479/999): loss=12.841777518414775, w0=0.7208786624910873, w1=0.7208786624910873\n",
            "Gradient Descent(480/999): loss=12.84177751836978, w0=0.7208844483141584, w1=0.7208844483141584\n",
            "Gradient Descent(481/999): loss=12.841777518327044, w0=0.7208788101981952, w1=0.7208788101981952\n",
            "Gradient Descent(482/999): loss=12.841777518286468, w0=0.7208843043778865, w1=0.7208843043778865\n",
            "Gradient Descent(483/999): loss=12.841777518247937, w0=0.7208789504598974, w1=0.7208789504598974\n",
            "Gradient Descent(484/999): loss=12.841777518211352, w0=0.7208841676969455, w1=0.7208841676969455\n",
            "Gradient Descent(485/999): loss=12.841777518176608, w0=0.720879083651491, w1=0.720879083651491\n",
            "Gradient Descent(486/999): loss=12.841777518143616, w0=0.7208840379056192, w1=0.7208840379056192\n",
            "Gradient Descent(487/999): loss=12.841777518112284, w0=0.720879210129356, w1=0.720879210129356\n",
            "Gradient Descent(488/999): loss=12.841777518082534, w0=0.7208839146566258, w1=0.7208839146566258\n",
            "Gradient Descent(489/999): loss=12.84177751805428, w0=0.7208793302319081, w1=0.7208793302319081\n",
            "Gradient Descent(490/999): loss=12.841777518027454, w0=0.7208837976201891, w1=0.7208837976201891\n",
            "Gradient Descent(491/999): loss=12.84177751800198, w0=0.7208794442805048, w1=0.7208794442805048\n",
            "Gradient Descent(492/999): loss=12.84177751797779, w0=0.7208836864831554, w1=0.7208836864831554\n",
            "Gradient Descent(493/999): loss=12.841777517954817, w0=0.7208795525803052, w1=0.7208795525803052\n",
            "Gradient Descent(494/999): loss=12.841777517933004, w0=0.720883580948156, w1=0.720883580948156\n",
            "Gradient Descent(495/999): loss=12.84177751791229, w0=0.7208796554210865, w1=0.7208796554210865\n",
            "Gradient Descent(496/999): loss=12.84177751789262, w0=0.7208834807328118, w1=0.7208834807328118\n",
            "Gradient Descent(497/999): loss=12.841777517873938, w0=0.7208797530780187, w1=0.7208797530780187\n",
            "Gradient Descent(498/999): loss=12.841777517856208, w0=0.7208833855689775, w1=0.7208833855689775\n",
            "Gradient Descent(499/999): loss=12.841777517839361, w0=0.7208798458124017, w1=0.7208798458124017\n",
            "Gradient Descent(500/999): loss=12.84177751782337, w0=0.7208832952020238, w1=0.7208832952020238\n",
            "Gradient Descent(501/999): loss=12.841777517808183, w0=0.7208799338723646, w1=0.7208799338723646\n",
            "Gradient Descent(502/999): loss=12.841777517793757, w0=0.7208832093901564, w1=0.7208832093901564\n",
            "Gradient Descent(503/999): loss=12.841777517780065, w0=0.7208800174935285, w1=0.7208800174935285\n",
            "Gradient Descent(504/999): loss=12.84177751776706, w0=0.7208831279037692, w1=0.7208831279037692\n",
            "Gradient Descent(505/999): loss=12.841777517754707, w0=0.720880096899638, w1=0.720880096899638\n",
            "Gradient Descent(506/999): loss=12.84177751774298, w0=0.7208830505248296, w1=0.7208830505248296\n",
            "Gradient Descent(507/999): loss=12.841777517731849, w0=0.7208801723031596, w1=0.7208801723031596\n",
            "Gradient Descent(508/999): loss=12.841777517721269, w0=0.7208829770462953, w1=0.7208829770462953\n",
            "Gradient Descent(509/999): loss=12.841777517711233, w0=0.7208802439058498, w1=0.7208802439058498\n",
            "Gradient Descent(510/999): loss=12.841777517701692, w0=0.7208829072715605, w1=0.7208829072715605\n",
            "Gradient Descent(511/999): loss=12.841777517692643, w0=0.7208803118992954, w1=0.7208803118992954\n",
            "Gradient Descent(512/999): loss=12.841777517684038, w0=0.7208828410139294, w1=0.7208828410139294\n",
            "Gradient Descent(513/999): loss=12.841777517675881, w0=0.7208803764654259, w1=0.7208803764654259\n",
            "Gradient Descent(514/999): loss=12.841777517668126, w0=0.7208827780961169, w1=0.7208827780961169\n",
            "Gradient Descent(515/999): loss=12.841777517660763, w0=0.7208804377770005, w1=0.7208804377770005\n",
            "Gradient Descent(516/999): loss=12.84177751765377, w0=0.7208827183497742, w1=0.7208827183497742\n",
            "Gradient Descent(517/999): loss=12.841777517647131, w0=0.7208804959980704, w1=0.7208804959980704\n",
            "Gradient Descent(518/999): loss=12.841777517640828, w0=0.7208826616150383, w1=0.7208826616150383\n",
            "Gradient Descent(519/999): loss=12.841777517634842, w0=0.720880551284417, w1=0.720880551284417\n",
            "Gradient Descent(520/999): loss=12.841777517629158, w0=0.7208826077401049, w1=0.7208826077401049\n",
            "Gradient Descent(521/999): loss=12.841777517623754, w0=0.7208806037839693, w1=0.7208806037839693\n",
            "Gradient Descent(522/999): loss=12.84177751761863, w0=0.720882556580821, w1=0.720882556580821\n",
            "Gradient Descent(523/999): loss=12.841777517613766, w0=0.7208806536372007, w1=0.7208806536372007\n",
            "Gradient Descent(524/999): loss=12.841777517609144, w0=0.7208825080003002, w1=0.7208825080003002\n",
            "Gradient Descent(525/999): loss=12.841777517604754, w0=0.7208807009775023, w1=0.7208807009775023\n",
            "Gradient Descent(526/999): loss=12.841777517600585, w0=0.7208824618685558, w1=0.7208824618685558\n",
            "Gradient Descent(527/999): loss=12.841777517596624, w0=0.7208807459315429, w1=0.7208807459315429\n",
            "Gradient Descent(528/999): loss=12.841777517592872, w0=0.7208824180621534, w1=0.7208824180621534\n",
            "Gradient Descent(529/999): loss=12.841777517589303, w0=0.7208807886196054, w1=0.7208807886196054\n",
            "Gradient Descent(530/999): loss=12.841777517585905, w0=0.7208823764638805, w1=0.7208823764638805\n",
            "Gradient Descent(531/999): loss=12.841777517582695, w0=0.7208808291559099, w1=0.7208808291559099\n",
            "Gradient Descent(532/999): loss=12.841777517579635, w0=0.7208823369624332, w1=0.7208823369624332\n",
            "Gradient Descent(533/999): loss=12.841777517576737, w0=0.720880867648919, w1=0.720880867648919\n",
            "Gradient Descent(534/999): loss=12.841777517573975, w0=0.7208822994521177, w1=0.7208822994521177\n",
            "Gradient Descent(535/999): loss=12.84177751757136, w0=0.7208809042016283, w1=0.7208809042016283\n",
            "Gradient Descent(536/999): loss=12.841777517568877, w0=0.7208822638325677, w1=0.7208822638325677\n",
            "Gradient Descent(537/999): loss=12.841777517566515, w0=0.7208809389118417, w1=0.7208809389118417\n",
            "Gradient Descent(538/999): loss=12.841777517564276, w0=0.7208822300084764, w1=0.7208822300084764\n",
            "Gradient Descent(539/999): loss=12.841777517562152, w0=0.7208809718724329, w1=0.7208809718724329\n",
            "Gradient Descent(540/999): loss=12.841777517560125, w0=0.7208821978893408, w1=0.7208821978893408\n",
            "Gradient Descent(541/999): loss=12.841777517558207, w0=0.7208810031715945, w1=0.7208810031715945\n",
            "Gradient Descent(542/999): loss=12.841777517556386, w0=0.7208821673892198, w1=0.7208821673892198\n",
            "Gradient Descent(543/999): loss=12.84177751755466, w0=0.7208810328930735, w1=0.7208810328930735\n",
            "Gradient Descent(544/999): loss=12.841777517553016, w0=0.720882138426505, w1=0.720882138426505\n",
            "Gradient Descent(545/999): loss=12.841777517551455, w0=0.720881061116395, w1=0.720881061116395\n",
            "Gradient Descent(546/999): loss=12.841777517549975, w0=0.7208821109237006, w1=0.7208821109237006\n",
            "Gradient Descent(547/999): loss=12.841777517548568, w0=0.7208810879170764, w1=0.7208810879170764\n",
            "Gradient Descent(548/999): loss=12.84177751754723, w0=0.7208820848072176, w1=0.7208820848072176\n",
            "Gradient Descent(549/999): loss=12.841777517545962, w0=0.720881113366828, w1=0.720881113366828\n",
            "Gradient Descent(550/999): loss=12.841777517544758, w0=0.7208820600071765, w1=0.7208820600071765\n",
            "Gradient Descent(551/999): loss=12.841777517543616, w0=0.7208811375337454, w1=0.7208811375337454\n",
            "Gradient Descent(552/999): loss=12.841777517542528, w0=0.7208820364572197, w1=0.7208820364572197\n",
            "Gradient Descent(553/999): loss=12.841777517541498, w0=0.7208811604824918, w1=0.7208811604824918\n",
            "Gradient Descent(554/999): loss=12.841777517540518, w0=0.7208820140943352, w1=0.7208820140943352\n",
            "Gradient Descent(555/999): loss=12.841777517539587, w0=0.7208811822744711, w1=0.7208811822744711\n",
            "Gradient Descent(556/999): loss=12.841777517538702, w0=0.7208819928586864, w1=0.7208819928586864\n",
            "Gradient Descent(557/999): loss=12.841777517537865, w0=0.720881202967992, w1=0.720881202967992\n",
            "Gradient Descent(558/999): loss=12.841777517537068, w0=0.7208819726934534, w1=0.7208819726934534\n",
            "Gradient Descent(559/999): loss=12.84177751753631, w0=0.7208812226184238, w1=0.7208812226184238\n",
            "Gradient Descent(560/999): loss=12.841777517535593, w0=0.7208819535446803, w1=0.7208819535446803\n",
            "Gradient Descent(561/999): loss=12.841777517534908, w0=0.7208812412783452, w1=0.7208812412783452\n",
            "Gradient Descent(562/999): loss=12.841777517534261, w0=0.7208819353611308, w1=0.7208819353611308\n",
            "Gradient Descent(563/999): loss=12.84177751753365, w0=0.7208812589976842, w1=0.7208812589976842\n",
            "Gradient Descent(564/999): loss=12.841777517533068, w0=0.7208819180941513, w1=0.7208819180941513\n",
            "Gradient Descent(565/999): loss=12.841777517532506, w0=0.7208812758238525, w1=0.7208812758238525\n",
            "Gradient Descent(566/999): loss=12.841777517531987, w0=0.7208819016975406, w1=0.7208819016975406\n",
            "Gradient Descent(567/999): loss=12.841777517531485, w0=0.7208812918018718, w1=0.7208812918018718\n",
            "Gradient Descent(568/999): loss=12.841777517531009, w0=0.7208818861274265, w1=0.7208818861274265\n",
            "Gradient Descent(569/999): loss=12.84177751753056, w0=0.7208813069744942, w1=0.7208813069744942\n",
            "Gradient Descent(570/999): loss=12.84177751753013, w0=0.7208818713421482, w1=0.7208818713421482\n",
            "Gradient Descent(571/999): loss=12.84177751752972, w0=0.720881321382317, w1=0.720881321382317\n",
            "Gradient Descent(572/999): loss=12.841777517529334, w0=0.7208818573021448, w1=0.7208818573021448\n",
            "Gradient Descent(573/999): loss=12.841777517528973, w0=0.7208813350638911, w1=0.7208813350638911\n",
            "Gradient Descent(574/999): loss=12.841777517528625, w0=0.7208818439698496, w1=0.7208818439698496\n",
            "Gradient Descent(575/999): loss=12.841777517528294, w0=0.7208813480558243, w1=0.7208813480558243\n",
            "Gradient Descent(576/999): loss=12.841777517527978, w0=0.7208818313095892, w1=0.7208818313095892\n",
            "Gradient Descent(577/999): loss=12.841777517527678, w0=0.720881360392879, w1=0.720881360392879\n",
            "Gradient Descent(578/999): loss=12.8417775175274, w0=0.720881819287489, w1=0.720881819287489\n",
            "Gradient Descent(579/999): loss=12.841777517527126, w0=0.7208813721080652, w1=0.7208813721080652\n",
            "Gradient Descent(580/999): loss=12.841777517526872, w0=0.7208818078713813, w1=0.7208818078713813\n",
            "Gradient Descent(581/999): loss=12.841777517526628, w0=0.7208813832327295, w1=0.7208813832327295\n",
            "Gradient Descent(582/999): loss=12.8417775175264, w0=0.7208817970307202, w1=0.7208817970307202\n",
            "Gradient Descent(583/999): loss=12.841777517526186, w0=0.7208813937966378, w1=0.7208813937966378\n",
            "Gradient Descent(584/999): loss=12.841777517525971, w0=0.7208817867364994, w1=0.7208817867364994\n",
            "Gradient Descent(585/999): loss=12.841777517525776, w0=0.7208814038280561, w1=0.7208814038280561\n",
            "Gradient Descent(586/999): loss=12.841777517525589, w0=0.7208817769611748, w1=0.7208817769611748\n",
            "Gradient Descent(587/999): loss=12.841777517525413, w0=0.7208814133538249, w1=0.7208814133538249\n",
            "Gradient Descent(588/999): loss=12.841777517525243, w0=0.7208817676785906, w1=0.7208817676785906\n",
            "Gradient Descent(589/999): loss=12.841777517525083, w0=0.7208814223994328, w1=0.7208814223994328\n",
            "Gradient Descent(590/999): loss=12.841777517524934, w0=0.7208817588639093, w1=0.7208817588639093\n",
            "Gradient Descent(591/999): loss=12.841777517524788, w0=0.7208814309890829, w1=0.7208814309890829\n",
            "Gradient Descent(592/999): loss=12.841777517524655, w0=0.7208817504935457, w1=0.7208817504935457\n",
            "Gradient Descent(593/999): loss=12.841777517524518, w0=0.7208814391457582, w1=0.7208814391457582\n",
            "Gradient Descent(594/999): loss=12.841777517524394, w0=0.7208817425451033, w1=0.7208817425451033\n",
            "Gradient Descent(595/999): loss=12.841777517524276, w0=0.7208814468912838, w1=0.7208814468912838\n",
            "Gradient Descent(596/999): loss=12.841777517524163, w0=0.7208817349973144, w1=0.7208817349973144\n",
            "Gradient Descent(597/999): loss=12.84177751752406, w0=0.720881454246384, w1=0.720881454246384\n",
            "Gradient Descent(598/999): loss=12.84177751752396, w0=0.7208817278299836, w1=0.7208817278299836\n",
            "Gradient Descent(599/999): loss=12.841777517523862, w0=0.7208814612307389, w1=0.7208814612307389\n",
            "Gradient Descent(600/999): loss=12.841777517523775, w0=0.7208817210239331, w1=0.7208817210239331\n",
            "Gradient Descent(601/999): loss=12.841777517523687, w0=0.7208814678630368, w1=0.7208814678630368\n",
            "Gradient Descent(602/999): loss=12.841777517523605, w0=0.7208817145609523, w1=0.7208817145609523\n",
            "Gradient Descent(603/999): loss=12.841777517523527, w0=0.7208814741610232, w1=0.7208814741610232\n",
            "Gradient Descent(604/999): loss=12.841777517523454, w0=0.7208817084237481, w1=0.7208817084237481\n",
            "Gradient Descent(605/999): loss=12.841777517523381, w0=0.7208814801415498, w1=0.7208814801415498\n",
            "Gradient Descent(606/999): loss=12.84177751752332, w0=0.7208817025958991, w1=0.7208817025958991\n",
            "Gradient Descent(607/999): loss=12.841777517523253, w0=0.7208814858206188, w1=0.7208814858206188\n",
            "Gradient Descent(608/999): loss=12.841777517523196, w0=0.7208816970618118, w1=0.7208816970618118\n",
            "Gradient Descent(609/999): loss=12.841777517523138, w0=0.7208814912134256, w1=0.7208814912134256\n",
            "Gradient Descent(610/999): loss=12.84177751752308, w0=0.7208816918066788, w1=0.7208816918066788\n",
            "Gradient Descent(611/999): loss=12.84177751752303, w0=0.7208814963343997, w1=0.7208814963343997\n",
            "Gradient Descent(612/999): loss=12.841777517522983, w0=0.7208816868164388, w1=0.7208816868164388\n",
            "Gradient Descent(613/999): loss=12.841777517522937, w0=0.720881501197243, w1=0.720881501197243\n",
            "Gradient Descent(614/999): loss=12.841777517522896, w0=0.7208816820777398, w1=0.7208816820777398\n",
            "Gradient Descent(615/999): loss=12.841777517522853, w0=0.7208815058149671, w1=0.7208815058149671\n",
            "Gradient Descent(616/999): loss=12.841777517522813, w0=0.7208816775779021, w1=0.7208816775779021\n",
            "Gradient Descent(617/999): loss=12.841777517522772, w0=0.7208815101999279, w1=0.7208815101999279\n",
            "Gradient Descent(618/999): loss=12.841777517522742, w0=0.7208816733048857, w1=0.7208816733048857\n",
            "Gradient Descent(619/999): loss=12.841777517522704, w0=0.7208815143638577, w1=0.7208815143638577\n",
            "Gradient Descent(620/999): loss=12.841777517522674, w0=0.7208816692472575, w1=0.7208816692472575\n",
            "Gradient Descent(621/999): loss=12.841777517522644, w0=0.7208815183178983, w1=0.7208815183178983\n",
            "Gradient Descent(622/999): loss=12.84177751752261, w0=0.7208816653941602, w1=0.7208816653941602\n",
            "Gradient Descent(623/999): loss=12.841777517522592, w0=0.7208815220726292, w1=0.7208815220726292\n",
            "Gradient Descent(624/999): loss=12.841777517522559, w0=0.7208816617352842, w1=0.7208816617352842\n",
            "Gradient Descent(625/999): loss=12.841777517522534, w0=0.7208815256380973, w1=0.7208815256380973\n",
            "Gradient Descent(626/999): loss=12.841777517522507, w0=0.7208816582608397, w1=0.7208816582608397\n",
            "Gradient Descent(627/999): loss=12.841777517522486, w0=0.7208815290238422, w1=0.7208815290238422\n",
            "Gradient Descent(628/999): loss=12.841777517522468, w0=0.7208816549615299, w1=0.7208816549615299\n",
            "Gradient Descent(629/999): loss=12.841777517522447, w0=0.7208815322389234, w1=0.7208815322389234\n",
            "Gradient Descent(630/999): loss=12.841777517522425, w0=0.7208816518285268, w1=0.7208816518285268\n",
            "Gradient Descent(631/999): loss=12.841777517522404, w0=0.7208815352919435, w1=0.7208815352919435\n",
            "Gradient Descent(632/999): loss=12.84177751752239, w0=0.7208816488534477, w1=0.7208816488534477\n",
            "Gradient Descent(633/999): loss=12.841777517522374, w0=0.7208815381910715, w1=0.7208815381910715\n",
            "Gradient Descent(634/999): loss=12.841777517522358, w0=0.7208816460283322, w1=0.7208816460283322\n",
            "Gradient Descent(635/999): loss=12.841777517522344, w0=0.7208815409440642, w1=0.7208815409440642\n",
            "Gradient Descent(636/999): loss=12.841777517522328, w0=0.7208816433456208, w1=0.7208816433456208\n",
            "Gradient Descent(637/999): loss=12.841777517522317, w0=0.7208815435582884, w1=0.7208815435582884\n",
            "Gradient Descent(638/999): loss=12.841777517522301, w0=0.7208816407981355, w1=0.7208816407981355\n",
            "Gradient Descent(639/999): loss=12.841777517522296, w0=0.7208815460407384, w1=0.7208815460407384\n",
            "Gradient Descent(640/999): loss=12.84177751752228, w0=0.7208816383790604, w1=0.7208816383790604\n",
            "Gradient Descent(641/999): loss=12.841777517522265, w0=0.7208815483980567, w1=0.7208815483980567\n",
            "Gradient Descent(642/999): loss=12.841777517522258, w0=0.7208816360819224, w1=0.7208816360819224\n",
            "Gradient Descent(643/999): loss=12.84177751752225, w0=0.7208815506365508, w1=0.7208815506365508\n",
            "Gradient Descent(644/999): loss=12.84177751752224, w0=0.7208816339005751, w1=0.7208816339005751\n",
            "Gradient Descent(645/999): loss=12.841777517522234, w0=0.72088155276221, w1=0.72088155276221\n",
            "Gradient Descent(646/999): loss=12.841777517522223, w0=0.7208816318291823, w1=0.7208816318291823\n",
            "Gradient Descent(647/999): loss=12.841777517522214, w0=0.720881554780722, w1=0.720881554780722\n",
            "Gradient Descent(648/999): loss=12.841777517522207, w0=0.7208816298622012, w1=0.7208816298622012\n",
            "Gradient Descent(649/999): loss=12.841777517522198, w0=0.7208815566974878, w1=0.7208815566974878\n",
            "Gradient Descent(650/999): loss=12.841777517522193, w0=0.7208816279943687, w1=0.7208816279943687\n",
            "Gradient Descent(651/999): loss=12.841777517522189, w0=0.7208815585176361, w1=0.7208815585176361\n",
            "Gradient Descent(652/999): loss=12.841777517522177, w0=0.7208816262206872, w1=0.7208816262206872\n",
            "Gradient Descent(653/999): loss=12.841777517522171, w0=0.7208815602460371, w1=0.7208815602460371\n",
            "Gradient Descent(654/999): loss=12.841777517522171, w0=0.7208816245364107, w1=0.7208816245364107\n",
            "Gradient Descent(655/999): loss=12.841777517522168, w0=0.7208815618873154, w1=0.7208815618873154\n",
            "Gradient Descent(656/999): loss=12.84177751752216, w0=0.7208816229370328, w1=0.7208816229370328\n",
            "Gradient Descent(657/999): loss=12.841777517522152, w0=0.7208815634458625, w1=0.7208815634458625\n",
            "Gradient Descent(658/999): loss=12.841777517522146, w0=0.7208816214182742, w1=0.7208816214182742\n",
            "Gradient Descent(659/999): loss=12.841777517522146, w0=0.7208815649258485, w1=0.7208815649258485\n",
            "Gradient Descent(660/999): loss=12.841777517522141, w0=0.7208816199760709, w1=0.7208816199760709\n",
            "Gradient Descent(661/999): loss=12.841777517522134, w0=0.7208815663312336, w1=0.7208815663312336\n",
            "Gradient Descent(662/999): loss=12.84177751752213, w0=0.7208816186065642, w1=0.7208816186065642\n",
            "Gradient Descent(663/999): loss=12.84177751752213, w0=0.7208815676657779, w1=0.7208815676657779\n",
            "Gradient Descent(664/999): loss=12.841777517522125, w0=0.7208816173060896, w1=0.7208816173060896\n",
            "Gradient Descent(665/999): loss=12.841777517522125, w0=0.7208815689330526, w1=0.7208815689330526\n",
            "Gradient Descent(666/999): loss=12.84177751752212, w0=0.7208816160711674, w1=0.7208816160711674\n",
            "Gradient Descent(667/999): loss=12.84177751752212, w0=0.7208815701364483, w1=0.7208815701364483\n",
            "Gradient Descent(668/999): loss=12.841777517522116, w0=0.7208816148984933, w1=0.7208816148984933\n",
            "Gradient Descent(669/999): loss=12.841777517522116, w0=0.720881571279185, w1=0.720881571279185\n",
            "Gradient Descent(670/999): loss=12.84177751752211, w0=0.7208816137849298, w1=0.7208816137849298\n",
            "Gradient Descent(671/999): loss=12.841777517522104, w0=0.7208815723643203, w1=0.7208815723643203\n",
            "Gradient Descent(672/999): loss=12.841777517522104, w0=0.7208816127274971, w1=0.7208816127274971\n",
            "Gradient Descent(673/999): loss=12.841777517522104, w0=0.7208815733947576, w1=0.7208815733947576\n",
            "Gradient Descent(674/999): loss=12.8417775175221, w0=0.7208816117233658, w1=0.7208816117233658\n",
            "Gradient Descent(675/999): loss=12.8417775175221, w0=0.7208815743732542, w1=0.7208815743732542\n",
            "Gradient Descent(676/999): loss=12.8417775175221, w0=0.7208816107698495, w1=0.7208816107698495\n",
            "Gradient Descent(677/999): loss=12.8417775175221, w0=0.7208815753024281, w1=0.7208815753024281\n",
            "Gradient Descent(678/999): loss=12.841777517522095, w0=0.7208816098643965, w1=0.7208816098643965\n",
            "Gradient Descent(679/999): loss=12.841777517522095, w0=0.7208815761847657, w1=0.7208815761847657\n",
            "Gradient Descent(680/999): loss=12.841777517522095, w0=0.7208816090045843, w1=0.7208816090045843\n",
            "Gradient Descent(681/999): loss=12.841777517522095, w0=0.7208815770226276, w1=0.7208815770226276\n",
            "Gradient Descent(682/999): loss=12.84177751752209, w0=0.7208816081881124, w1=0.7208816081881124\n",
            "Gradient Descent(683/999): loss=12.84177751752209, w0=0.7208815778182556, w1=0.7208815778182556\n",
            "Gradient Descent(684/999): loss=12.84177751752209, w0=0.7208816074127959, w1=0.7208816074127959\n",
            "Gradient Descent(685/999): loss=12.841777517522083, w0=0.7208815785737789, w1=0.7208815785737789\n",
            "Gradient Descent(686/999): loss=12.841777517522083, w0=0.7208816066765606, w1=0.7208816066765606\n",
            "Gradient Descent(687/999): loss=12.841777517522083, w0=0.7208815792912188, w1=0.7208815792912188\n",
            "Gradient Descent(688/999): loss=12.841777517522083, w0=0.7208816059774363, w1=0.7208816059774363\n",
            "Gradient Descent(689/999): loss=12.841777517522079, w0=0.720881579972495, w1=0.720881579972495\n",
            "Gradient Descent(690/999): loss=12.841777517522079, w0=0.7208816053135524, w1=0.7208816053135524\n",
            "Gradient Descent(691/999): loss=12.841777517522079, w0=0.7208815806194306, w1=0.7208815806194306\n",
            "Gradient Descent(692/999): loss=12.841777517522079, w0=0.7208816046831326, w1=0.7208816046831326\n",
            "Gradient Descent(693/999): loss=12.841777517522079, w0=0.7208815812337563, w1=0.7208815812337563\n",
            "Gradient Descent(694/999): loss=12.841777517522074, w0=0.7208816040844901, w1=0.7208816040844901\n",
            "Gradient Descent(695/999): loss=12.841777517522079, w0=0.7208815818171159, w1=0.7208815818171159\n",
            "Gradient Descent(696/999): loss=12.841777517522074, w0=0.7208816035160232, w1=0.7208816035160232\n",
            "Gradient Descent(697/999): loss=12.841777517522074, w0=0.7208815823710705, w1=0.7208815823710705\n",
            "Gradient Descent(698/999): loss=12.841777517522074, w0=0.7208816029762105, w1=0.7208816029762105\n",
            "Gradient Descent(699/999): loss=12.841777517522074, w0=0.7208815828971022, w1=0.7208815828971022\n",
            "Gradient Descent(700/999): loss=12.841777517522074, w0=0.720881602463608, w1=0.720881602463608\n",
            "Gradient Descent(701/999): loss=12.841777517522074, w0=0.7208815833966183, w1=0.7208815833966183\n",
            "Gradient Descent(702/999): loss=12.841777517522074, w0=0.7208816019768441, w1=0.7208816019768441\n",
            "Gradient Descent(703/999): loss=12.841777517522068, w0=0.7208815838709556, w1=0.7208815838709556\n",
            "Gradient Descent(704/999): loss=12.841777517522074, w0=0.7208816015146163, w1=0.7208816015146163\n",
            "Gradient Descent(705/999): loss=12.841777517522074, w0=0.720881584321383, w1=0.720881584321383\n",
            "Gradient Descent(706/999): loss=12.841777517522068, w0=0.7208816010756879, w1=0.7208816010756879\n",
            "Gradient Descent(707/999): loss=12.841777517522074, w0=0.720881584749106, w1=0.720881584749106\n",
            "Gradient Descent(708/999): loss=12.841777517522074, w0=0.7208816006588842, w1=0.7208816006588842\n",
            "Gradient Descent(709/999): loss=12.841777517522068, w0=0.7208815851552691, w1=0.7208815851552691\n",
            "Gradient Descent(710/999): loss=12.841777517522068, w0=0.7208816002630902, w1=0.7208816002630902\n",
            "Gradient Descent(711/999): loss=12.841777517522068, w0=0.7208815855409587, w1=0.7208815855409587\n",
            "Gradient Descent(712/999): loss=12.841777517522068, w0=0.7208815998872469, w1=0.7208815998872469\n",
            "Gradient Descent(713/999): loss=12.841777517522068, w0=0.7208815859072072, w1=0.7208815859072072\n",
            "Gradient Descent(714/999): loss=12.841777517522074, w0=0.7208815995303482, w1=0.7208815995303482\n",
            "Gradient Descent(715/999): loss=12.841777517522074, w0=0.7208815862549945, w1=0.7208815862549945\n",
            "Gradient Descent(716/999): loss=12.841777517522068, w0=0.7208815991914398, w1=0.7208815991914398\n",
            "Gradient Descent(717/999): loss=12.841777517522068, w0=0.7208815865852508, w1=0.7208815865852508\n",
            "Gradient Descent(718/999): loss=12.841777517522068, w0=0.7208815988696147, w1=0.7208815988696147\n",
            "Gradient Descent(719/999): loss=12.841777517522068, w0=0.7208815868988601, w1=0.7208815868988601\n",
            "Gradient Descent(720/999): loss=12.841777517522068, w0=0.7208815985640116, w1=0.7208815985640116\n",
            "Gradient Descent(721/999): loss=12.841777517522068, w0=0.7208815871966614, w1=0.7208815871966614\n",
            "Gradient Descent(722/999): loss=12.841777517522068, w0=0.7208815982738128, w1=0.7208815982738128\n",
            "Gradient Descent(723/999): loss=12.841777517522068, w0=0.7208815874794516, w1=0.7208815874794516\n",
            "Gradient Descent(724/999): loss=12.841777517522065, w0=0.720881597998242, w1=0.720881597998242\n",
            "Gradient Descent(725/999): loss=12.841777517522068, w0=0.7208815877479874, w1=0.7208815877479874\n",
            "Gradient Descent(726/999): loss=12.841777517522065, w0=0.7208815977365618, w1=0.7208815977365618\n",
            "Gradient Descent(727/999): loss=12.841777517522068, w0=0.7208815880029871, w1=0.7208815880029871\n",
            "Gradient Descent(728/999): loss=12.841777517522068, w0=0.720881597488072, w1=0.720881597488072\n",
            "Gradient Descent(729/999): loss=12.841777517522065, w0=0.7208815882451333, w1=0.7208815882451333\n",
            "Gradient Descent(730/999): loss=12.841777517522065, w0=0.7208815972521077, w1=0.7208815972521077\n",
            "Gradient Descent(731/999): loss=12.841777517522068, w0=0.7208815884750736, w1=0.7208815884750736\n",
            "Gradient Descent(732/999): loss=12.841777517522068, w0=0.7208815970280376, w1=0.7208815970280376\n",
            "Gradient Descent(733/999): loss=12.841777517522068, w0=0.7208815886934233, w1=0.7208815886934233\n",
            "Gradient Descent(734/999): loss=12.841777517522068, w0=0.720881596815262, w1=0.720881596815262\n",
            "Gradient Descent(735/999): loss=12.841777517522065, w0=0.7208815889007669, w1=0.7208815889007669\n",
            "Gradient Descent(736/999): loss=12.841777517522068, w0=0.7208815966132117, w1=0.7208815966132117\n",
            "Gradient Descent(737/999): loss=12.841777517522068, w0=0.720881589097659, w1=0.720881589097659\n",
            "Gradient Descent(738/999): loss=12.841777517522068, w0=0.7208815964213461, w1=0.7208815964213461\n",
            "Gradient Descent(739/999): loss=12.841777517522065, w0=0.7208815892846265, w1=0.7208815892846265\n",
            "Gradient Descent(740/999): loss=12.841777517522065, w0=0.7208815962391517, w1=0.7208815962391517\n",
            "Gradient Descent(741/999): loss=12.841777517522068, w0=0.7208815894621696, w1=0.7208815894621696\n",
            "Gradient Descent(742/999): loss=12.841777517522065, w0=0.720881596066141, w1=0.720881596066141\n",
            "Gradient Descent(743/999): loss=12.841777517522065, w0=0.7208815896307635, w1=0.7208815896307635\n",
            "Gradient Descent(744/999): loss=12.841777517522065, w0=0.7208815959018513, w1=0.7208815959018513\n",
            "Gradient Descent(745/999): loss=12.841777517522068, w0=0.7208815897908591, w1=0.7208815897908591\n",
            "Gradient Descent(746/999): loss=12.841777517522065, w0=0.7208815957458428, w1=0.7208815957458428\n",
            "Gradient Descent(747/999): loss=12.841777517522065, w0=0.7208815899428848, w1=0.7208815899428848\n",
            "Gradient Descent(748/999): loss=12.841777517522065, w0=0.7208815955976983, w1=0.7208815955976983\n",
            "Gradient Descent(749/999): loss=12.841777517522065, w0=0.7208815900872472, w1=0.7208815900872472\n",
            "Gradient Descent(750/999): loss=12.841777517522065, w0=0.7208815954570212, w1=0.7208815954570212\n",
            "Gradient Descent(751/999): loss=12.841777517522065, w0=0.720881590224333, w1=0.720881590224333\n",
            "Gradient Descent(752/999): loss=12.841777517522065, w0=0.7208815953234352, w1=0.7208815953234352\n",
            "Gradient Descent(753/999): loss=12.841777517522068, w0=0.7208815903545086, w1=0.7208815903545086\n",
            "Gradient Descent(754/999): loss=12.841777517522065, w0=0.7208815951965828, w1=0.7208815951965828\n",
            "Gradient Descent(755/999): loss=12.841777517522068, w0=0.7208815904781226, w1=0.7208815904781226\n",
            "Gradient Descent(756/999): loss=12.841777517522065, w0=0.7208815950761245, w1=0.7208815950761245\n",
            "Gradient Descent(757/999): loss=12.841777517522065, w0=0.7208815905955058, w1=0.7208815905955058\n",
            "Gradient Descent(758/999): loss=12.841777517522068, w0=0.720881594961738, w1=0.720881594961738\n",
            "Gradient Descent(759/999): loss=12.841777517522065, w0=0.720881590706972, w1=0.720881590706972\n",
            "Gradient Descent(760/999): loss=12.841777517522065, w0=0.7208815948531175, w1=0.7208815948531175\n",
            "Gradient Descent(761/999): loss=12.841777517522065, w0=0.7208815908128196, w1=0.7208815908128196\n",
            "Gradient Descent(762/999): loss=12.841777517522068, w0=0.7208815947499722, w1=0.7208815947499722\n",
            "Gradient Descent(763/999): loss=12.841777517522068, w0=0.7208815909133317, w1=0.7208815909133317\n",
            "Gradient Descent(764/999): loss=12.841777517522065, w0=0.720881594652026, w1=0.720881594652026\n",
            "Gradient Descent(765/999): loss=12.841777517522065, w0=0.7208815910087775, w1=0.7208815910087775\n",
            "Gradient Descent(766/999): loss=12.841777517522065, w0=0.7208815945590169, w1=0.7208815945590169\n",
            "Gradient Descent(767/999): loss=12.841777517522065, w0=0.720881591099412, w1=0.720881591099412\n",
            "Gradient Descent(768/999): loss=12.841777517522065, w0=0.7208815944706962, w1=0.7208815944706962\n",
            "Gradient Descent(769/999): loss=12.841777517522065, w0=0.7208815911854779, w1=0.7208815911854779\n",
            "Gradient Descent(770/999): loss=12.841777517522065, w0=0.7208815943868275, w1=0.7208815943868275\n",
            "Gradient Descent(771/999): loss=12.841777517522065, w0=0.7208815912672055, w1=0.7208815912672055\n",
            "Gradient Descent(772/999): loss=12.841777517522065, w0=0.7208815943071862, w1=0.7208815943071862\n",
            "Gradient Descent(773/999): loss=12.841777517522065, w0=0.7208815913448137, w1=0.7208815913448137\n",
            "Gradient Descent(774/999): loss=12.841777517522068, w0=0.7208815942315594, w1=0.7208815942315594\n",
            "Gradient Descent(775/999): loss=12.841777517522065, w0=0.7208815914185098, w1=0.7208815914185098\n",
            "Gradient Descent(776/999): loss=12.841777517522065, w0=0.7208815941597446, w1=0.7208815941597446\n",
            "Gradient Descent(777/999): loss=12.841777517522065, w0=0.7208815914884912, w1=0.7208815914884912\n",
            "Gradient Descent(778/999): loss=12.841777517522065, w0=0.7208815940915498, w1=0.7208815940915498\n",
            "Gradient Descent(779/999): loss=12.841777517522065, w0=0.7208815915549451, w1=0.7208815915549451\n",
            "Gradient Descent(780/999): loss=12.841777517522065, w0=0.7208815940267923, w1=0.7208815940267923\n",
            "Gradient Descent(781/999): loss=12.841777517522068, w0=0.7208815916180493, w1=0.7208815916180493\n",
            "Gradient Descent(782/999): loss=12.841777517522065, w0=0.7208815939652992, w1=0.7208815939652992\n",
            "Gradient Descent(783/999): loss=12.841777517522065, w0=0.7208815916779726, w1=0.7208815916779726\n",
            "Gradient Descent(784/999): loss=12.841777517522068, w0=0.7208815939069056, w1=0.7208815939069056\n",
            "Gradient Descent(785/999): loss=12.841777517522065, w0=0.7208815917348754, w1=0.7208815917348754\n",
            "Gradient Descent(786/999): loss=12.841777517522065, w0=0.7208815938514556, w1=0.7208815938514556\n",
            "Gradient Descent(787/999): loss=12.841777517522065, w0=0.7208815917889099, w1=0.7208815917889099\n",
            "Gradient Descent(788/999): loss=12.841777517522065, w0=0.7208815937988006, w1=0.7208815937988006\n",
            "Gradient Descent(789/999): loss=12.841777517522065, w0=0.7208815918402206, w1=0.7208815918402206\n",
            "Gradient Descent(790/999): loss=12.841777517522065, w0=0.7208815937487998, w1=0.7208815937487998\n",
            "Gradient Descent(791/999): loss=12.841777517522065, w0=0.7208815918889451, w1=0.7208815918889451\n",
            "Gradient Descent(792/999): loss=12.841777517522065, w0=0.7208815937013191, w1=0.7208815937013191\n",
            "Gradient Descent(793/999): loss=12.841777517522065, w0=0.7208815919352135, w1=0.7208815919352135\n",
            "Gradient Descent(794/999): loss=12.841777517522065, w0=0.720881593656232, w1=0.720881593656232\n",
            "Gradient Descent(795/999): loss=12.841777517522065, w0=0.7208815919791496, w1=0.7208815919791496\n",
            "Gradient Descent(796/999): loss=12.841777517522065, w0=0.7208815936134175, w1=0.7208815936134175\n",
            "Gradient Descent(797/999): loss=12.841777517522065, w0=0.720881592020871, w1=0.720881592020871\n",
            "Gradient Descent(798/999): loss=12.841777517522068, w0=0.7208815935727612, w1=0.7208815935727612\n",
            "Gradient Descent(799/999): loss=12.841777517522065, w0=0.7208815920604894, w1=0.7208815920604894\n",
            "Gradient Descent(800/999): loss=12.841777517522065, w0=0.7208815935341543, w1=0.7208815935341543\n",
            "Gradient Descent(801/999): loss=12.841777517522065, w0=0.7208815920981108, w1=0.7208815920981108\n",
            "Gradient Descent(802/999): loss=12.841777517522065, w0=0.7208815934974934, w1=0.7208815934974934\n",
            "Gradient Descent(803/999): loss=12.841777517522065, w0=0.7208815921338357, w1=0.7208815921338357\n",
            "Gradient Descent(804/999): loss=12.841777517522065, w0=0.7208815934626804, w1=0.7208815934626804\n",
            "Gradient Descent(805/999): loss=12.841777517522068, w0=0.7208815921677599, w1=0.7208815921677599\n",
            "Gradient Descent(806/999): loss=12.841777517522065, w0=0.7208815934296222, w1=0.7208815934296222\n",
            "Gradient Descent(807/999): loss=12.841777517522065, w0=0.7208815921999742, w1=0.7208815921999742\n",
            "Gradient Descent(808/999): loss=12.841777517522065, w0=0.7208815933982304, w1=0.7208815933982304\n",
            "Gradient Descent(809/999): loss=12.841777517522065, w0=0.7208815922305646, w1=0.7208815922305646\n",
            "Gradient Descent(810/999): loss=12.841777517522065, w0=0.720881593368421, w1=0.720881593368421\n",
            "Gradient Descent(811/999): loss=12.841777517522065, w0=0.720881592259613, w1=0.720881592259613\n",
            "Gradient Descent(812/999): loss=12.841777517522065, w0=0.720881593340114, w1=0.720881593340114\n",
            "Gradient Descent(813/999): loss=12.841777517522065, w0=0.7208815922871973, w1=0.7208815922871973\n",
            "Gradient Descent(814/999): loss=12.841777517522068, w0=0.720881593313234, w1=0.720881593313234\n",
            "Gradient Descent(815/999): loss=12.841777517522065, w0=0.7208815923133911, w1=0.7208815923133911\n",
            "Gradient Descent(816/999): loss=12.841777517522065, w0=0.7208815932877088, w1=0.7208815932877088\n",
            "Gradient Descent(817/999): loss=12.841777517522065, w0=0.7208815923382647, w1=0.7208815923382647\n",
            "Gradient Descent(818/999): loss=12.841777517522065, w0=0.7208815932634703, w1=0.7208815932634703\n",
            "Gradient Descent(819/999): loss=12.841777517522065, w0=0.7208815923618843, w1=0.7208815923618843\n",
            "Gradient Descent(820/999): loss=12.841777517522065, w0=0.7208815932404536, w1=0.7208815932404536\n",
            "Gradient Descent(821/999): loss=12.841777517522065, w0=0.7208815923843135, w1=0.7208815923843135\n",
            "Gradient Descent(822/999): loss=12.841777517522065, w0=0.720881593218597, w1=0.720881593218597\n",
            "Gradient Descent(823/999): loss=12.841777517522068, w0=0.7208815924056121, w1=0.7208815924056121\n",
            "Gradient Descent(824/999): loss=12.841777517522065, w0=0.7208815931978422, w1=0.7208815931978422\n",
            "Gradient Descent(825/999): loss=12.841777517522065, w0=0.7208815924258372, w1=0.7208815924258372\n",
            "Gradient Descent(826/999): loss=12.841777517522065, w0=0.7208815931781335, w1=0.7208815931781335\n",
            "Gradient Descent(827/999): loss=12.841777517522065, w0=0.7208815924450427, w1=0.7208815924450427\n",
            "Gradient Descent(828/999): loss=12.841777517522065, w0=0.7208815931594182, w1=0.7208815931594182\n",
            "Gradient Descent(829/999): loss=12.841777517522065, w0=0.7208815924632801, w1=0.7208815924632801\n",
            "Gradient Descent(830/999): loss=12.841777517522065, w0=0.7208815931416465, w1=0.7208815931416465\n",
            "Gradient Descent(831/999): loss=12.841777517522065, w0=0.7208815924805982, w1=0.7208815924805982\n",
            "Gradient Descent(832/999): loss=12.841777517522068, w0=0.7208815931247704, w1=0.7208815931247704\n",
            "Gradient Descent(833/999): loss=12.841777517522065, w0=0.7208815924970434, w1=0.7208815924970434\n",
            "Gradient Descent(834/999): loss=12.841777517522068, w0=0.720881593108745, w1=0.720881593108745\n",
            "Gradient Descent(835/999): loss=12.841777517522065, w0=0.7208815925126597, w1=0.7208815925126597\n",
            "Gradient Descent(836/999): loss=12.841777517522065, w0=0.7208815930935274, w1=0.7208815930935274\n",
            "Gradient Descent(837/999): loss=12.841777517522068, w0=0.7208815925274888, w1=0.7208815925274888\n",
            "Gradient Descent(838/999): loss=12.841777517522065, w0=0.720881593079077, w1=0.720881593079077\n",
            "Gradient Descent(839/999): loss=12.841777517522065, w0=0.7208815925415702, w1=0.7208815925415702\n",
            "Gradient Descent(840/999): loss=12.841777517522065, w0=0.720881593065355, w1=0.720881593065355\n",
            "Gradient Descent(841/999): loss=12.841777517522065, w0=0.7208815925549419, w1=0.7208815925549419\n",
            "Gradient Descent(842/999): loss=12.841777517522065, w0=0.7208815930523247, w1=0.7208815930523247\n",
            "Gradient Descent(843/999): loss=12.841777517522068, w0=0.7208815925676396, w1=0.7208815925676396\n",
            "Gradient Descent(844/999): loss=12.841777517522065, w0=0.7208815930399511, w1=0.7208815930399511\n",
            "Gradient Descent(845/999): loss=12.841777517522065, w0=0.7208815925796973, w1=0.7208815925796973\n",
            "Gradient Descent(846/999): loss=12.841777517522065, w0=0.7208815930282012, w1=0.7208815930282012\n",
            "Gradient Descent(847/999): loss=12.841777517522065, w0=0.7208815925911473, w1=0.7208815925911473\n",
            "Gradient Descent(848/999): loss=12.841777517522065, w0=0.7208815930170436, w1=0.7208815930170436\n",
            "Gradient Descent(849/999): loss=12.841777517522065, w0=0.7208815926020201, w1=0.7208815926020201\n",
            "Gradient Descent(850/999): loss=12.841777517522068, w0=0.7208815930064484, w1=0.7208815930064484\n",
            "Gradient Descent(851/999): loss=12.841777517522068, w0=0.7208815926123447, w1=0.7208815926123447\n",
            "Gradient Descent(852/999): loss=12.841777517522065, w0=0.7208815929963873, w1=0.7208815929963873\n",
            "Gradient Descent(853/999): loss=12.841777517522065, w0=0.720881592622149, w1=0.720881592622149\n",
            "Gradient Descent(854/999): loss=12.841777517522065, w0=0.7208815929868333, w1=0.7208815929868333\n",
            "Gradient Descent(855/999): loss=12.841777517522065, w0=0.720881592631459, w1=0.720881592631459\n",
            "Gradient Descent(856/999): loss=12.841777517522065, w0=0.7208815929777609, w1=0.7208815929777609\n",
            "Gradient Descent(857/999): loss=12.841777517522065, w0=0.7208815926402998, w1=0.7208815926402998\n",
            "Gradient Descent(858/999): loss=12.841777517522065, w0=0.7208815929691459, w1=0.7208815929691459\n",
            "Gradient Descent(859/999): loss=12.841777517522065, w0=0.7208815926486949, w1=0.7208815926486949\n",
            "Gradient Descent(860/999): loss=12.841777517522065, w0=0.7208815929609651, w1=0.7208815929609651\n",
            "Gradient Descent(861/999): loss=12.841777517522065, w0=0.7208815926566668, w1=0.7208815926566668\n",
            "Gradient Descent(862/999): loss=12.841777517522065, w0=0.7208815929531966, w1=0.7208815929531966\n",
            "Gradient Descent(863/999): loss=12.841777517522068, w0=0.720881592664237, w1=0.720881592664237\n",
            "Gradient Descent(864/999): loss=12.841777517522065, w0=0.7208815929458198, w1=0.7208815929458198\n",
            "Gradient Descent(865/999): loss=12.841777517522065, w0=0.7208815926714256, w1=0.7208815926714256\n",
            "Gradient Descent(866/999): loss=12.841777517522065, w0=0.7208815929388147, w1=0.7208815929388147\n",
            "Gradient Descent(867/999): loss=12.841777517522065, w0=0.7208815926782518, w1=0.7208815926782518\n",
            "Gradient Descent(868/999): loss=12.841777517522065, w0=0.7208815929321628, w1=0.7208815929321628\n",
            "Gradient Descent(869/999): loss=12.841777517522068, w0=0.720881592684734, w1=0.720881592684734\n",
            "Gradient Descent(870/999): loss=12.841777517522065, w0=0.7208815929258462, w1=0.7208815929258462\n",
            "Gradient Descent(871/999): loss=12.841777517522065, w0=0.7208815926908893, w1=0.7208815926908893\n",
            "Gradient Descent(872/999): loss=12.841777517522068, w0=0.720881592919848, w1=0.720881592919848\n",
            "Gradient Descent(873/999): loss=12.841777517522065, w0=0.7208815926967344, w1=0.7208815926967344\n",
            "Gradient Descent(874/999): loss=12.841777517522065, w0=0.7208815929141521, w1=0.7208815929141521\n",
            "Gradient Descent(875/999): loss=12.841777517522065, w0=0.7208815927022848, w1=0.7208815927022848\n",
            "Gradient Descent(876/999): loss=12.841777517522065, w0=0.7208815929087433, w1=0.7208815929087433\n",
            "Gradient Descent(877/999): loss=12.841777517522068, w0=0.7208815927075555, w1=0.7208815927075555\n",
            "Gradient Descent(878/999): loss=12.841777517522065, w0=0.7208815929036072, w1=0.7208815929036072\n",
            "Gradient Descent(879/999): loss=12.841777517522065, w0=0.7208815927125605, w1=0.7208815927125605\n",
            "Gradient Descent(880/999): loss=12.841777517522065, w0=0.7208815928987299, w1=0.7208815928987299\n",
            "Gradient Descent(881/999): loss=12.841777517522065, w0=0.7208815927173133, w1=0.7208815927173133\n",
            "Gradient Descent(882/999): loss=12.841777517522065, w0=0.7208815928940985, w1=0.7208815928940985\n",
            "Gradient Descent(883/999): loss=12.841777517522065, w0=0.7208815927218264, w1=0.7208815927218264\n",
            "Gradient Descent(884/999): loss=12.841777517522065, w0=0.7208815928897007, w1=0.7208815928897007\n",
            "Gradient Descent(885/999): loss=12.841777517522065, w0=0.7208815927261121, w1=0.7208815927261121\n",
            "Gradient Descent(886/999): loss=12.841777517522065, w0=0.7208815928855243, w1=0.7208815928855243\n",
            "Gradient Descent(887/999): loss=12.841777517522065, w0=0.7208815927301818, w1=0.7208815927301818\n",
            "Gradient Descent(888/999): loss=12.841777517522065, w0=0.7208815928815584, w1=0.7208815928815584\n",
            "Gradient Descent(889/999): loss=12.841777517522065, w0=0.7208815927340464, w1=0.7208815927340464\n",
            "Gradient Descent(890/999): loss=12.841777517522065, w0=0.7208815928777925, w1=0.7208815928777925\n",
            "Gradient Descent(891/999): loss=12.841777517522065, w0=0.7208815927377162, w1=0.7208815927377162\n",
            "Gradient Descent(892/999): loss=12.841777517522065, w0=0.7208815928742164, w1=0.7208815928742164\n",
            "Gradient Descent(893/999): loss=12.841777517522065, w0=0.7208815927412009, w1=0.7208815927412009\n",
            "Gradient Descent(894/999): loss=12.841777517522065, w0=0.7208815928708208, w1=0.7208815928708208\n",
            "Gradient Descent(895/999): loss=12.841777517522065, w0=0.7208815927445099, w1=0.7208815927445099\n",
            "Gradient Descent(896/999): loss=12.841777517522065, w0=0.7208815928675961, w1=0.7208815928675961\n",
            "Gradient Descent(897/999): loss=12.841777517522068, w0=0.7208815927476523, w1=0.7208815927476523\n",
            "Gradient Descent(898/999): loss=12.841777517522065, w0=0.720881592864534, w1=0.720881592864534\n",
            "Gradient Descent(899/999): loss=12.841777517522065, w0=0.7208815927506361, w1=0.7208815927506361\n",
            "Gradient Descent(900/999): loss=12.841777517522065, w0=0.7208815928616263, w1=0.7208815928616263\n",
            "Gradient Descent(901/999): loss=12.841777517522065, w0=0.7208815927534695, w1=0.7208815927534695\n",
            "Gradient Descent(902/999): loss=12.841777517522065, w0=0.7208815928588652, w1=0.7208815928588652\n",
            "Gradient Descent(903/999): loss=12.841777517522065, w0=0.7208815927561603, w1=0.7208815927561603\n",
            "Gradient Descent(904/999): loss=12.841777517522065, w0=0.7208815928562432, w1=0.7208815928562432\n",
            "Gradient Descent(905/999): loss=12.841777517522065, w0=0.7208815927587153, w1=0.7208815927587153\n",
            "Gradient Descent(906/999): loss=12.841777517522065, w0=0.7208815928537534, w1=0.7208815928537534\n",
            "Gradient Descent(907/999): loss=12.841777517522065, w0=0.7208815927611416, w1=0.7208815927611416\n",
            "Gradient Descent(908/999): loss=12.841777517522065, w0=0.720881592851389, w1=0.720881592851389\n",
            "Gradient Descent(909/999): loss=12.841777517522065, w0=0.7208815927634457, w1=0.7208815927634457\n",
            "Gradient Descent(910/999): loss=12.841777517522065, w0=0.7208815928491439, w1=0.7208815928491439\n",
            "Gradient Descent(911/999): loss=12.841777517522065, w0=0.7208815927656333, w1=0.7208815927656333\n",
            "Gradient Descent(912/999): loss=12.841777517522065, w0=0.720881592847012, w1=0.720881592847012\n",
            "Gradient Descent(913/999): loss=12.841777517522068, w0=0.7208815927677109, w1=0.7208815927677109\n",
            "Gradient Descent(914/999): loss=12.841777517522068, w0=0.7208815928449875, w1=0.7208815928449875\n",
            "Gradient Descent(915/999): loss=12.841777517522068, w0=0.7208815927696837, w1=0.7208815927696837\n",
            "Gradient Descent(916/999): loss=12.841777517522065, w0=0.7208815928430651, w1=0.7208815928430651\n",
            "Gradient Descent(917/999): loss=12.841777517522065, w0=0.7208815927715571, w1=0.7208815927715571\n",
            "Gradient Descent(918/999): loss=12.841777517522068, w0=0.7208815928412395, w1=0.7208815928412395\n",
            "Gradient Descent(919/999): loss=12.841777517522065, w0=0.7208815927733359, w1=0.7208815927733359\n",
            "Gradient Descent(920/999): loss=12.841777517522065, w0=0.7208815928395061, w1=0.7208815928395061\n",
            "Gradient Descent(921/999): loss=12.841777517522065, w0=0.7208815927750252, w1=0.7208815927750252\n",
            "Gradient Descent(922/999): loss=12.841777517522065, w0=0.72088159283786, w1=0.72088159283786\n",
            "Gradient Descent(923/999): loss=12.841777517522065, w0=0.7208815927766292, w1=0.7208815927766292\n",
            "Gradient Descent(924/999): loss=12.841777517522065, w0=0.7208815928362968, w1=0.7208815928362968\n",
            "Gradient Descent(925/999): loss=12.841777517522065, w0=0.7208815927781526, w1=0.7208815927781526\n",
            "Gradient Descent(926/999): loss=12.841777517522065, w0=0.7208815928348125, w1=0.7208815928348125\n",
            "Gradient Descent(927/999): loss=12.841777517522065, w0=0.720881592779599, w1=0.720881592779599\n",
            "Gradient Descent(928/999): loss=12.841777517522065, w0=0.7208815928334029, w1=0.7208815928334029\n",
            "Gradient Descent(929/999): loss=12.841777517522065, w0=0.7208815927809725, w1=0.7208815927809725\n",
            "Gradient Descent(930/999): loss=12.841777517522065, w0=0.7208815928320644, w1=0.7208815928320644\n",
            "Gradient Descent(931/999): loss=12.841777517522065, w0=0.7208815927822767, w1=0.7208815927822767\n",
            "Gradient Descent(932/999): loss=12.841777517522065, w0=0.7208815928307936, w1=0.7208815928307936\n",
            "Gradient Descent(933/999): loss=12.841777517522065, w0=0.7208815927835153, w1=0.7208815927835153\n",
            "Gradient Descent(934/999): loss=12.841777517522065, w0=0.7208815928295866, w1=0.7208815928295866\n",
            "Gradient Descent(935/999): loss=12.841777517522065, w0=0.7208815927846913, w1=0.7208815927846913\n",
            "Gradient Descent(936/999): loss=12.841777517522068, w0=0.7208815928284406, w1=0.7208815928284406\n",
            "Gradient Descent(937/999): loss=12.841777517522065, w0=0.7208815927858082, w1=0.7208815927858082\n",
            "Gradient Descent(938/999): loss=12.841777517522065, w0=0.7208815928273521, w1=0.7208815928273521\n",
            "Gradient Descent(939/999): loss=12.841777517522065, w0=0.7208815927868689, w1=0.7208815927868689\n",
            "Gradient Descent(940/999): loss=12.841777517522068, w0=0.7208815928263186, w1=0.7208815928263186\n",
            "Gradient Descent(941/999): loss=12.841777517522065, w0=0.720881592787876, w1=0.720881592787876\n",
            "Gradient Descent(942/999): loss=12.841777517522065, w0=0.7208815928253373, w1=0.7208815928253373\n",
            "Gradient Descent(943/999): loss=12.841777517522065, w0=0.7208815927888322, w1=0.7208815927888322\n",
            "Gradient Descent(944/999): loss=12.841777517522065, w0=0.7208815928244053, w1=0.7208815928244053\n",
            "Gradient Descent(945/999): loss=12.841777517522065, w0=0.7208815927897405, w1=0.7208815927897405\n",
            "Gradient Descent(946/999): loss=12.841777517522065, w0=0.7208815928235203, w1=0.7208815928235203\n",
            "Gradient Descent(947/999): loss=12.841777517522065, w0=0.7208815927906029, w1=0.7208815927906029\n",
            "Gradient Descent(948/999): loss=12.841777517522065, w0=0.7208815928226798, w1=0.7208815928226798\n",
            "Gradient Descent(949/999): loss=12.841777517522065, w0=0.7208815927914218, w1=0.7208815927914218\n",
            "Gradient Descent(950/999): loss=12.841777517522065, w0=0.7208815928218819, w1=0.7208815928218819\n",
            "Gradient Descent(951/999): loss=12.841777517522065, w0=0.7208815927921994, w1=0.7208815927921994\n",
            "Gradient Descent(952/999): loss=12.841777517522065, w0=0.7208815928211241, w1=0.7208815928211241\n",
            "Gradient Descent(953/999): loss=12.841777517522065, w0=0.720881592792938, w1=0.720881592792938\n",
            "Gradient Descent(954/999): loss=12.841777517522065, w0=0.7208815928204044, w1=0.7208815928204044\n",
            "Gradient Descent(955/999): loss=12.841777517522065, w0=0.7208815927936392, w1=0.7208815927936392\n",
            "Gradient Descent(956/999): loss=12.841777517522068, w0=0.7208815928197212, w1=0.7208815928197212\n",
            "Gradient Descent(957/999): loss=12.841777517522065, w0=0.720881592794305, w1=0.720881592794305\n",
            "Gradient Descent(958/999): loss=12.841777517522065, w0=0.7208815928190724, w1=0.7208815928190724\n",
            "Gradient Descent(959/999): loss=12.841777517522065, w0=0.7208815927949372, w1=0.7208815927949372\n",
            "Gradient Descent(960/999): loss=12.841777517522065, w0=0.7208815928184562, w1=0.7208815928184562\n",
            "Gradient Descent(961/999): loss=12.841777517522065, w0=0.7208815927955377, w1=0.7208815927955377\n",
            "Gradient Descent(962/999): loss=12.841777517522065, w0=0.7208815928178712, w1=0.7208815928178712\n",
            "Gradient Descent(963/999): loss=12.841777517522065, w0=0.7208815927961076, w1=0.7208815927961076\n",
            "Gradient Descent(964/999): loss=12.841777517522065, w0=0.7208815928173157, w1=0.7208815928173157\n",
            "Gradient Descent(965/999): loss=12.841777517522065, w0=0.7208815927966491, w1=0.7208815927966491\n",
            "Gradient Descent(966/999): loss=12.841777517522065, w0=0.720881592816788, w1=0.720881592816788\n",
            "Gradient Descent(967/999): loss=12.841777517522065, w0=0.7208815927971634, w1=0.7208815927971634\n",
            "Gradient Descent(968/999): loss=12.841777517522065, w0=0.7208815928162869, w1=0.7208815928162869\n",
            "Gradient Descent(969/999): loss=12.841777517522065, w0=0.7208815927976515, w1=0.7208815927976515\n",
            "Gradient Descent(970/999): loss=12.841777517522065, w0=0.7208815928158113, w1=0.7208815928158113\n",
            "Gradient Descent(971/999): loss=12.841777517522065, w0=0.7208815927981151, w1=0.7208815927981151\n",
            "Gradient Descent(972/999): loss=12.841777517522065, w0=0.7208815928153595, w1=0.7208815928153595\n",
            "Gradient Descent(973/999): loss=12.841777517522065, w0=0.7208815927985553, w1=0.7208815927985553\n",
            "Gradient Descent(974/999): loss=12.841777517522068, w0=0.7208815928149305, w1=0.7208815928149305\n",
            "Gradient Descent(975/999): loss=12.841777517522068, w0=0.7208815927989735, w1=0.7208815927989735\n",
            "Gradient Descent(976/999): loss=12.841777517522065, w0=0.720881592814523, w1=0.720881592814523\n",
            "Gradient Descent(977/999): loss=12.841777517522065, w0=0.7208815927993705, w1=0.7208815927993705\n",
            "Gradient Descent(978/999): loss=12.841777517522065, w0=0.7208815928141361, w1=0.7208815928141361\n",
            "Gradient Descent(979/999): loss=12.841777517522065, w0=0.7208815927997475, w1=0.7208815927997475\n",
            "Gradient Descent(980/999): loss=12.841777517522065, w0=0.7208815928137687, w1=0.7208815928137687\n",
            "Gradient Descent(981/999): loss=12.841777517522065, w0=0.7208815928001054, w1=0.7208815928001054\n",
            "Gradient Descent(982/999): loss=12.841777517522065, w0=0.7208815928134199, w1=0.7208815928134199\n",
            "Gradient Descent(983/999): loss=12.841777517522065, w0=0.7208815928004454, w1=0.7208815928004454\n",
            "Gradient Descent(984/999): loss=12.841777517522065, w0=0.7208815928130887, w1=0.7208815928130887\n",
            "Gradient Descent(985/999): loss=12.841777517522065, w0=0.7208815928007681, w1=0.7208815928007681\n",
            "Gradient Descent(986/999): loss=12.841777517522068, w0=0.7208815928127742, w1=0.7208815928127742\n",
            "Gradient Descent(987/999): loss=12.841777517522068, w0=0.7208815928010747, w1=0.7208815928010747\n",
            "Gradient Descent(988/999): loss=12.841777517522065, w0=0.7208815928124755, w1=0.7208815928124755\n",
            "Gradient Descent(989/999): loss=12.841777517522068, w0=0.7208815928013657, w1=0.7208815928013657\n",
            "Gradient Descent(990/999): loss=12.841777517522065, w0=0.7208815928121919, w1=0.7208815928121919\n",
            "Gradient Descent(991/999): loss=12.841777517522065, w0=0.720881592801642, w1=0.720881592801642\n",
            "Gradient Descent(992/999): loss=12.841777517522065, w0=0.7208815928119225, w1=0.7208815928119225\n",
            "Gradient Descent(993/999): loss=12.841777517522065, w0=0.7208815928019046, w1=0.7208815928019046\n",
            "Gradient Descent(994/999): loss=12.841777517522065, w0=0.7208815928116667, w1=0.7208815928116667\n",
            "Gradient Descent(995/999): loss=12.841777517522065, w0=0.7208815928021538, w1=0.7208815928021538\n",
            "Gradient Descent(996/999): loss=12.841777517522065, w0=0.7208815928114238, w1=0.7208815928114238\n",
            "Gradient Descent(997/999): loss=12.841777517522065, w0=0.7208815928023906, w1=0.7208815928023906\n",
            "Gradient Descent(998/999): loss=12.841777517522065, w0=0.7208815928111931, w1=0.7208815928111931\n",
            "Gradient Descent(999/999): loss=12.841777517522065, w0=0.7208815928026153, w1=0.7208815928026153\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72088159, 0.72088159])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "g, cost = gradientDescent(X, y, theta, alpha, iters)\n",
        "g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdJDLNDxrk6k"
      },
      "source": [
        "Finally we can compute the cost (error) of the trained model using our fitted parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "v9V8zgw7rk6k",
        "outputId": "38604243-1aca-4a43-a8ee-b5f513fd9770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.841777517522065"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "computeCost(X, y, g)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C61uNnCrk6k"
      },
      "source": [
        "Now let's plot the linear model along with the data to visually see how well it fits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ufxevoQlrk6l",
        "outputId": "910fca17-9f1c-45c0-aef9-bd396b6c28ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Predicted Profit vs. Population Size')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "x = np.linspace(data.Population.min(), data.Population.max(), 100)\n",
        "f = g[0] + (g[1] * x)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.plot(x, f, 'r', label='Prediction')\n",
        "ax.scatter(data.Population, data.Profit, label='Traning Data')\n",
        "ax.legend(loc=2)\n",
        "ax.set_xlabel('Population')\n",
        "ax.set_ylabel('Profit')\n",
        "ax.set_title('Predicted Profit vs. Population Size')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBXqhOycrk6l"
      },
      "source": [
        "Looks pretty good!  Since the gradient decent function also outputs a vector with the cost at each training iteration, we can plot that as well.  Notice that the cost always decreases - this is an example of a convex optimization problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FImALhU5rk6m",
        "outputId": "d1e864ca-d857-4dc2-eb6d-eae24ff22449",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Error vs. Training Epoch')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(12,8))\n",
        "ax.plot(np.arange(iters), cost, 'r')\n",
        "ax.set_xlabel('Iterations')\n",
        "ax.set_ylabel('Cost')\n",
        "ax.set_title('Error vs. Training Epoch')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "Exercise 4_Linear_Regression.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}