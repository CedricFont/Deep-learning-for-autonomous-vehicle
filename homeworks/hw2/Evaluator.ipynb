{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evaluator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fPFXjAVFIKnh"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import platform\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uGxnwhvlwMiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97f48fb8-358e-43ac-cd66-7f46861aff79"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/vita-epfl/DLAV-2022.git\n",
        "path = os.getcwd() + '/DLAV-2022/homeworks/hw2/test_batch'"
      ],
      "metadata": {
        "id": "SwxcJW9wI9fp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "083650b2-6dc1-4df3-96db-61bf6b158a47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DLAV-2022'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 68 (delta 22), reused 54 (delta 14), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the location of the saved weight relative to this notebook. Assume that they are in the same directory\n",
        "### Path to Model Weights \n",
        "softmax_weights = 'drive/MyDrive/Colab Notebooks/softmax_weights.pkl' \n",
        "pytorch_weights = 'drive/MyDrive/Colab Notebooks/linearClassifier_pytorch.ckpt'"
      ],
      "metadata": {
        "id": "pZXQTJIKJE_S"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Copy your code from the Softmax Notebook to their corresponding function"
      ],
      "metadata": {
        "id": "mE6psT_aVPHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def softmax_loss_vectorized(W, X, y):\n",
        "    \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "    # regularization!                                                           #\n",
        "    #############################################################################\n",
        "    C, D, N = W.shape[1], W.shape[0], X.shape[0] # Nb. of classes\n",
        "    f_linear = W.T@X.T # CxD x DxN => CxN\n",
        "    f_linear = f_linear - np.max(f_linear,axis=0) # for more numerically stable solution (exp can overshoot max representable number)\n",
        "    S = np.divide(np.exp(f_linear),np.sum(np.exp(f_linear),axis=0)) # Softmax, CxN\n",
        "    y_one_hot = np.eye(C)[y].T # CxN\n",
        "\n",
        "    loss = np.mean(np.sum(S - y_one_hot,axis=0).T) # Vectorized, Nx1\n",
        "\n",
        "    # The grad of the cross-entropy softmax of each element in the batch w.r.t. w should be \n",
        "    # a matrix of size : DxC\n",
        "    dW = ((S - y_one_hot) @ X).T\n",
        "    #############################################################################\n",
        "    #                          END OF YOUR CODE                                 #\n",
        "    #############################################################################\n",
        "    \n",
        "    return loss, dW\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, num_iters=30000,\n",
        "                batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "          means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        \n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "        \n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            X_batch = None\n",
        "            y_batch = None\n",
        "\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            choice = np.random.choice(num_train,batch_size)\n",
        "            X_batch = X[choice]\n",
        "            y_batch = y[choice]\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            self.W -= learning_rate * grad\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "\n",
        "        return loss_history\n",
        "    \n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "          training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "          array of length N, and each element is an integer giving the predicted\n",
        "          class.\n",
        "        \"\"\"\n",
        "\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        f_linear = self.W.T@X.T # CxD x DxN => CxN\n",
        "        f_linear = f_linear - np.max(f_linear,axis=0) # To avoid numerical instabilities\n",
        "        y_pred = np.argmax(np.divide(np.exp(f_linear),np.sum(np.exp(f_linear),axis=0)),axis=0)\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative. \n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "          data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \n",
        "         e = y_batch - np.dot(X_batch, self.W) \n",
        "        \n",
        "        loss = np.dot(e.T, e)\n",
        "        grad = -np.dot(x_batch.T,e) / x_batch.shape[0]\n",
        "  \n",
        "        return loss, grad\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)\n",
        "        \n",
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)"
      ],
      "metadata": {
        "id": "gHnLX6-oIkWm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO:** Copy the model you created from the Pytorch Notebook"
      ],
      "metadata": {
        "id": "6chaH4G-Vfms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, n_feature, n_hidden, hidden_size, n_output):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Define 2 or more different layers of the neural network                      #\n",
        "        ################################################################################\n",
        "\n",
        "        # Input : n_features, output : n_output\n",
        "        self.layers = []\n",
        "        self.layers_params = []\n",
        "        # dimensions = np.linspace(n_feature,n_output,n_hidden + 3).astype(int)\n",
        "        dimensions = hidden_size * np.ones([n_hidden + 3]).astype(int)\n",
        "        dimensions[0], dimensions[-1] = n_feature, n_output\n",
        "        # Define as many hidden layers as required with same dimensions\n",
        "        for i in range(n_hidden + 2):\n",
        "          self.layers.append(torch.nn.Linear(dimensions[i],dimensions[i+1]))\n",
        "          self.layers_params += list(self.layers[i].parameters())\n",
        "\n",
        "        ################################################################################\n",
        "        #                              END OF YOUR CODE                                #\n",
        "        ################################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0),-1)\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Set up the forward pass that the input data will go through.                 #\n",
        "        # A good activation function betweent the layers is a ReLu function.           #\n",
        "        ################################################################################\n",
        "        for _, layer in enumerate(self.layers):\n",
        "          x = torch.nn.functional.relu(layer(x))\n",
        "        ################################################################################\n",
        "        #                              END OF YOUR CODE                                #\n",
        "        ################################################################################\n",
        "        return x"
      ],
      "metadata": {
        "id": "mSTfKTHEJBhy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO**: Follow the instructions in each of the following methods. **Note that these methods should return a 1-D array of size N where N is the number of data samples. The values should be the predicted classes [0,...,9].**\n",
        "\n"
      ],
      "metadata": {
        "id": "_UUbNTUAVsos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_usingPytorch(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Create your model                                                   #\n",
        "    # - Load your saved model                                               #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array (hint: return \"argmax\")         #\n",
        "    #########################################################################\n",
        "    # Load network\n",
        "    net = Net(n_feature=3072, n_hidden=3, hidden_size=1000, n_output=10)     # define the network\n",
        "    checkpoint = torch.load(\"drive/MyDrive/Colab Notebooks/linearClassifier_pytorch.ckpt\")\n",
        "    net.load_state_dict(checkpoint)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = torch.argmax(net(X))\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred.numpy()\n",
        "\n",
        "def predict_usingSoftmax(X):\n",
        "    #########################################################################\n",
        "    # TODO:                                                                 #\n",
        "    # - Load your saved model into the weights of Softmax                   #\n",
        "    # - Do the operation required to get the predictions                    #\n",
        "    # - Return predictions in a numpy array                                 #\n",
        "    #########################################################################\n",
        "    # Load model\n",
        "    with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'rb') as f:\n",
        "      W = pickle.load(f)\n",
        "    loaded_softmax = Softmax()\n",
        "    loaded_softmax.W = W.copy()\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = loaded_softmax.predict(X)\n",
        "    #########################################################################\n",
        "    #                       END OF YOUR CODE                                #\n",
        "    #########################################################################\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "bEKafMuaI4By"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method loads the test dataset to evaluate the model."
      ],
      "metadata": {
        "id": "q8dM8fj39OBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Read DATA\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "test_filename = path\n",
        "X,Y = load_CIFAR_batch(test_filename)"
      ],
      "metadata": {
        "id": "400u4eZNJAZq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet prepares the data for the different models. If you modify data manipulation in your notebooks, make sure to include them here. "
      ],
      "metadata": {
        "id": "AJ3mBYnx9TIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Manipulation\n",
        "\n",
        "mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "std = np.array([0.2023, 0.1994, 0.2010])\n",
        "X = np.divide(np.subtract( X/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
        "\n",
        "X_pytorch = torch.Tensor(np.moveaxis(X,-1,1))\n",
        "X_softmax = np.reshape(X, (X.shape[0], -1))\n",
        "X_softmax = np.hstack([X_softmax, np.ones((X_softmax.shape[0], 1))])\n"
      ],
      "metadata": {
        "id": "IEmU5KnwJPBY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Runs evaluation on the Pytorch and softmax model. **Be careful that *prediction_pytorch* and *prediction_softmax* are 1-D array of size N where N is the number of data samples. The values should be the predicted class [0,...,9]**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O2nQbKPL9c3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Run Prediction\n",
        "prediction_pytorch = predict_usingPytorch(X_pytorch)\n",
        "prediction_softmax = predict_usingSoftmax(X_softmax)\n",
        "\n",
        "## Run Evaluation\n",
        "acc_softmax = sum(prediction_softmax == Y)/len(X)\n",
        "acc_pytorch = sum(prediction_pytorch == Y)/len(X)\n",
        "print(\"Softmax= %f ... Pytorch= %f\"%(acc_softmax, acc_pytorch))"
      ],
      "metadata": {
        "id": "VKFPhm1wJjDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40d40cf0-232d-4c89-b9ff-b493ad5c58ec"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax= 0.379500 ... Pytorch= 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qroI8swROjZf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}