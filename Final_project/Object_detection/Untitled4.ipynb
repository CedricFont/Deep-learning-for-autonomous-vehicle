{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMr6bUXdeo+yP73h7T0y2FY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CedricFont/Deep-learning-for-autonomous-vehicle/blob/develop/Final_project/Object_detection/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XyBowCW15mXd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2"
      ],
      "metadata": {
        "id": "5k0TYIeL7VTm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spmallick/learnopencv.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xarEvS4G6TnT",
        "outputId": "0874e27b-b37d-4270-e5df-cacfd46c4490"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'learnopencv'...\n",
            "remote: Enumerating objects: 10158, done.\u001b[K\n",
            "remote: Total 10158 (delta 0), reused 0 (delta 0), pack-reused 10158\u001b[K\n",
            "Receiving objects: 100% (10158/10158), 1.22 GiB | 11.86 MiB/s, done.\n",
            "Resolving deltas: 100% (3023/3023), done.\n",
            "Checking out files: 100% (4489/4489), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants.\n",
        "INPUT_WIDTH = 640\n",
        "INPUT_HEIGHT = 640\n",
        "SCORE_THRESHOLD = 0.5\n",
        "NMS_THRESHOLD = 0.45\n",
        "CONFIDENCE_THRESHOLD = 0.45\n",
        "\n",
        "# Text parameters.\n",
        "FONT_FACE = cv2.FONT_HERSHEY_SIMPLEX\n",
        "FONT_SCALE = 0.7\n",
        "THICKNESS = 1\n",
        "\n",
        "# Colors\n",
        "BLACK  = (0,0,0)\n",
        "BLUE   = (255,178,50)\n",
        "YELLOW = (0,255,255)\n",
        "RED = (0,0,255)"
      ],
      "metadata": {
        "id": "l9Nxinz65oa2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "N90cSLzg9DJy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_label(input_image, label, left, top):\n",
        "    \"\"\"Draw text onto image at location.\"\"\"\n",
        "    \n",
        "    # Get text size.\n",
        "    text_size = cv2.getTextSize(label, FONT_FACE, FONT_SCALE, THICKNESS)\n",
        "    dim, baseline = text_size[0], text_size[1]\n",
        "    # Use text size to create a BLACK rectangle. \n",
        "    cv2.rectangle(input_image, (left, top), (left + dim[0], top + dim[1] + baseline), BLACK, cv2.FILLED);\n",
        "    # Display text inside the rectangle.\n",
        "    cv2.putText(input_image, label, (left, top + dim[1]), FONT_FACE, FONT_SCALE, YELLOW, THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "def pre_process(input_image, net):\n",
        "\t# Create a 4D blob from a frame.\n",
        "\tblob = cv2.dnn.blobFromImage(input_image, 1/255, (INPUT_WIDTH, INPUT_HEIGHT), [0,0,0], 1, crop=False)\n",
        "\n",
        "\t# Sets the input to the network.\n",
        "\tnet.setInput(blob)\n",
        "\n",
        "\t# Runs the forward pass to get output of the output layers.\n",
        "\toutput_layers = net.getUnconnectedOutLayersNames()\n",
        "\toutputs = net.forward(output_layers)\n",
        "\t# print(outputs[0].shape)\n",
        "\n",
        "\treturn outputs"
      ],
      "metadata": {
        "id": "wnxqkobS5qI_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def post_process(input_image, outputs):\n",
        "\t# Lists to hold respective values while unwrapping.\n",
        "\tclass_ids = []\n",
        "\tconfidences = []\n",
        "\tboxes = []\n",
        "\n",
        "\t# Rows.\n",
        "\trows = outputs[0].shape[1]\n",
        "\n",
        "\timage_height, image_width = input_image.shape[:2]\n",
        "\n",
        "\t# Resizing factor.\n",
        "\tx_factor = image_width / INPUT_WIDTH\n",
        "\ty_factor =  image_height / INPUT_HEIGHT\n",
        "\n",
        "\t# Iterate through 25200 detections.\n",
        "\tfor r in range(rows):\n",
        "\t\trow = outputs[0][0][r]\n",
        "\t\tconfidence = row[4]\n",
        "\n",
        "\t\t# Discard bad detections and continue.\n",
        "\t\tif confidence >= CONFIDENCE_THRESHOLD:\n",
        "\t\t\tclasses_scores = row[5:]\n",
        "\n",
        "\t\t\t# Get the index of max class score.\n",
        "\t\t\tclass_id = np.argmax(classes_scores)\n",
        "\n",
        "\t\t\t#  Continue if the class score is above threshold.\n",
        "\t\t\tif (classes_scores[class_id] > SCORE_THRESHOLD):\n",
        "\t\t\t\tconfidences.append(confidence)\n",
        "\t\t\t\tclass_ids.append(class_id)\n",
        "\n",
        "\t\t\t\tcx, cy, w, h = row[0], row[1], row[2], row[3]\n",
        "\n",
        "\t\t\t\tleft = int((cx - w/2) * x_factor)\n",
        "\t\t\t\ttop = int((cy - h/2) * y_factor)\n",
        "\t\t\t\twidth = int(w * x_factor)\n",
        "\t\t\t\theight = int(h * y_factor)\n",
        "\t\t\t  \n",
        "\t\t\t\tbox = np.array([left, top, width, height])\n",
        "\t\t\t\tboxes.append(box)\n",
        "\n",
        "\t# Perform non maximum suppression to eliminate redundant overlapping boxes with\n",
        "\t# lower confidences.\n",
        "\tindices = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
        "\tfor i in indices:\n",
        "\t\tbox = boxes[i]\n",
        "\t\tleft = box[0]\n",
        "\t\ttop = box[1]\n",
        "\t\twidth = box[2]\n",
        "\t\theight = box[3]\n",
        "\t\tcv2.rectangle(input_image, (left, top), (left + width, top + height), BLUE, 3*THICKNESS)\n",
        "\t\tlabel = \"{}:{:.2f}\".format(classes[class_ids[i]], confidences[i])\n",
        "\t\tdraw_label(input_image, label, left, top)\n",
        "\n",
        "\treturn input_image"
      ],
      "metadata": {
        "id": "J4nRc9tW5tzE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "ewFGG04U7N5X"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ls -a\n",
        "cd YOLOv5/models"
      ],
      "metadata": {
        "id": "Ub4cNeBNHJbB",
        "outputId": "10212aec-3de5-4f4d-cc57-61d452770fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "..\n",
            ".config\n",
            "learnopencv\n",
            "sample_data\n",
            "YOLOv5\n",
            "YOLOv5s.pt\n",
            "YOLOv5s.pt.1\n",
            "YOLOv5s.pt.2\n",
            "YOLOv5s.pt.3\n",
            "YOLOv5s.pt.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a\n",
        "%cd ..\n",
        "%ls -a"
      ],
      "metadata": {
        "id": "tPoY5LJ7L0Wx",
        "outputId": "3e232ecf-a077-4172-8172-f25a90f3730b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/         experimental.py  tf.py         yolov5m.yaml  yolov5x.yaml\n",
            "\u001b[01;34m..\u001b[0m/        \u001b[01;34mhub\u001b[0m/             yolo.py       yolov5n.yaml\n",
            "common.py  __init__.py      yolov5l.yaml  yolov5s.yaml\n",
            "/content/YOLOv5\n",
            "\u001b[0m\u001b[01;34m.\u001b[0m/               .dockerignore   hubconf.py               setup.cfg\n",
            "\u001b[01;34m..\u001b[0m/              export.py       LICENSE                  train.py\n",
            "CONTRIBUTING.md  \u001b[01;34m.git\u001b[0m/           \u001b[01;34mmodels\u001b[0m/                  tutorial.ipynb\n",
            "\u001b[01;34mdata\u001b[0m/            .gitattributes  .pre-commit-config.yaml  \u001b[01;34mutils\u001b[0m/\n",
            "detect.py        \u001b[01;34m.github\u001b[0m/        README.md                val.py\n",
            "Dockerfile       \u001b[01;32m.gitignore\u001b[0m*     \u001b[01;32mrequirements.txt\u001b[0m*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "id": "4_StrNwnMTex",
        "outputId": "4c4d5133-2f82-4f3a-950d-1f9eabfb537e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/   \u001b[01;34m.config\u001b[0m/      \u001b[01;34msample_data\u001b[0m/  YOLOv5s.pt    YOLOv5s.pt.2  YOLOv5s.pt.4\n",
            "\u001b[01;34m..\u001b[0m/  \u001b[01;34mlearnopencv\u001b[0m/  \u001b[01;34mYOLOv5\u001b[0m/       YOLOv5s.pt.1  YOLOv5s.pt.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository. \n",
        "!git clone https://github.com/ultralytics/YOLOv5\n",
        "\n",
        "%cd YOLOv5 # Install dependencies.\n",
        "!pip install -r requirements.txt  # install\n",
        "\n",
        "# Download .pt model.\n",
        "!wget https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt\n",
        "\n",
        "# %cd .. # Export to ONNX.\n",
        "!python export.py --weights models/YOLOv5s.pt --include onnx\n",
        "!python export.py --weights models/yolov5n.pt --include onnx"
      ],
      "metadata": {
        "id": "aN0zXoKm_rAh",
        "outputId": "42662bfd-7253-4613-99a9-077b755a0c6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'YOLOv5'...\n",
            "remote: Enumerating objects: 12824, done.\u001b[K\n",
            "remote: Total 12824 (delta 0), reused 0 (delta 0), pack-reused 12824\u001b[K\n",
            "Receiving objects: 100% (12824/12824), 11.75 MiB | 11.34 MiB/s, done.\n",
            "Resolving deltas: 100% (8914/8914), done.\n",
            "[Errno 2] No such file or directory: 'YOLOv5 # Install dependencies.'\n",
            "/content/YOLOv5\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.21.5)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Collecting PyYAML>=5.3.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.11.1+cu111)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.64.0)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (2.8.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
            "Collecting thop\n",
            "  Downloading thop-0.0.31.post2005241907-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (4.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.3.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.44.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.2.0)\n",
            "Installing collected packages: thop, PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-6.0 thop-0.0.31.post2005241907\n",
            "--2022-04-14 13:11:36--  https://github.com/ultralytics/YOLOv5/releases/download/v6.1/YOLOv5s.pt\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/76813c2d-b52b-47af-95fb-e92c1b0b2783?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220414T131137Z&X-Amz-Expires=300&X-Amz-Signature=60d2b80ab0c94497d83399ba2c958c83c20f42f392255caf76e2fe54fe595ebc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-04-14 13:11:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/76813c2d-b52b-47af-95fb-e92c1b0b2783?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220414%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220414T131137Z&X-Amz-Expires=300&X-Amz-Signature=60d2b80ab0c94497d83399ba2c958c83c20f42f392255caf76e2fe54fe595ebc&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14808437 (14M) [application/octet-stream]\n",
            "Saving to: ‚ÄòYOLOv5s.pt‚Äô\n",
            "\n",
            "YOLOv5s.pt          100%[===================>]  14.12M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-14 13:11:37 (110 MB/s) - ‚ÄòYOLOv5s.pt‚Äô saved [14808437/14808437]\n",
            "\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/YOLOv5s.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
            "YOLOv5 üöÄ v6.1-132-g014acde torch 1.10.0+cu111 CPU\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"export.py\", line 587, in <module>\n",
            "    main(opt)\n",
            "  File \"export.py\", line 582, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"export.py\", line 471, in run\n",
            "    model = attempt_load(weights, map_location=device, inplace=True, fuse=True)  # load FP32 model\n",
            "  File \"/content/YOLOv5/models/experimental.py\", line 96, in attempt_load\n",
            "    ckpt = torch.load(attempt_download(w), map_location=map_location)  # load\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 594, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 230, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 211, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'models/YOLOv5s.pt'\n",
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['models/yolov5n.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
            "YOLOv5 üöÄ v6.1-132-g014acde torch 1.10.0+cu111 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5n.pt to models/yolov5n.pt...\n",
            "100% 3.87M/3.87M [00:02<00:00, 1.83MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from models/yolov5n.pt with output shape (1, 25200, 85) (3.9 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m onnx not found and is required by YOLOv5, attempting auto-update...\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.11.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m ‚ö†Ô∏è \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.11.0...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success, saved as models/yolov5n.onnx (7.5 MB)\n",
            "\n",
            "Export complete (12.78s)\n",
            "Results saved to \u001b[1m/content/YOLOv5/models\u001b[0m\n",
            "Detect:          python detect.py --weights models/yolov5n.onnx\n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'models/yolov5n.onnx')\n",
            "Validate:        python val.py --weights models/yolov5n.onnx\n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "JBmS09WcNEqY",
        "outputId": "320a8383-38fc-4eca-97a7-e9001e19a7f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "id": "7DKzTqFjDn1R",
        "outputId": "1ec17ce8-f402-4dcc-c983-37b3a573e447",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/   \u001b[01;34m.config\u001b[0m/      \u001b[01;34msample_data\u001b[0m/  YOLOv5s.pt    YOLOv5s.pt.2  YOLOv5s.pt.4\n",
            "\u001b[01;34m..\u001b[0m/  \u001b[01;34mlearnopencv\u001b[0m/  \u001b[01;34mYOLOv5\u001b[0m/       YOLOv5s.pt.1  YOLOv5s.pt.3  YOLOv5s.pt.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file.\n",
        "from google.colab import files\n",
        "files.download('YOLOv5/models/yolov5n.onnx')"
      ],
      "metadata": {
        "id": "T3O2EffzDSU5",
        "outputId": "a6d4bb6c-2efd-4cee-fa5e-4e7d96e8b3d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f563431-a913-4625-b358-d6c6e40b485c\", \"yolov5n.onnx\", 7897041)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "id": "6D0f8EggPndv",
        "outputId": "dd88b4ab-fc3a-4346-abcc-4a5714409f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m.\u001b[0m/   \u001b[01;34m.config\u001b[0m/      \u001b[01;34msample_data\u001b[0m/  YOLOv5s.pt    YOLOv5s.pt.2  YOLOv5s.pt.4\n",
            "\u001b[01;34m..\u001b[0m/  \u001b[01;34mlearnopencv\u001b[0m/  \u001b[01;34mYOLOv5\u001b[0m/       YOLOv5s.pt.1  YOLOv5s.pt.3  YOLOv5s.pt.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load class names.\n",
        "classesFile = \"learnopencv/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python/coco.names\"\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "  classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Load image.\n",
        "frame = cv2.imread('sample.jpg')\n",
        "\n",
        "# Give the weight files to the model and load the network using them.\n",
        "modelWeights = \"learnopencv/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python/models/yolov5s.onnx\"\n",
        "net = cv2.dnn.readNet(modelWeights)"
      ],
      "metadata": {
        "id": "oP7FD3hx-ikO",
        "outputId": "a76f5562-fe46-43a3-8bb6-b74b6802618f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-c87cb7a499b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Give the weight files to the model and load the network using them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodelWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"learnopencv/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python/models/yolov5n.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/dnn/src/onnx/onnx_importer.cpp:110: error: (-215:Assertion failed) !tensor_proto.raw_data().empty() || !tensor_proto.float_data().empty() || !tensor_proto.double_data().empty() || !tensor_proto.int64_data().empty() in function 'getMatFromTensor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # grayscale image for face detection\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # get face region coordinates\n",
        "    faces = face_cascade.detectMultiScale(gray)\n",
        "    # get face bounding box for overlay\n",
        "    for (x,y,w,h) in faces:\n",
        "      bbox_array = cv2.rectangle(bbox_array,(x,y),(x+w,y+h),(255,0,0),2)\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "p2XYdENd-G5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load class names.\n",
        "classesFile = \"learnopencv/Object-Detection-using-YOLOv5-and-OpenCV-DNN-in-CPP-and-Python/coco.names\"\n",
        "classes = None\n",
        "with open(classesFile, 'rt') as f:\n",
        "  classes = f.read().rstrip('\\n').split('\\n')\n",
        "\n",
        "# Load image.\n",
        "# frame = cv2.imread('sample.jpg')\n",
        "\n",
        "# Give the weight files to the model and load the network using them.\n",
        "modelWeights = \"YOLOv5/models/yolov5n.onnx\"\n",
        "net = cv2.dnn.readNet(modelWeights)\n",
        "\n",
        "# Process image.\n",
        "detections = pre_process(frame, net)\n",
        "img = post_process(frame.copy(), detections)\n",
        "\n",
        "# Put efficiency information. The function getPerfProfile returns the overall time for inference(t) and the timings for each of the layers(in layersTimes)\n",
        "t, _ = net.getPerfProfile()\n",
        "label = 'Inference time: %.2f ms' % (t * 1000.0 / cv2.getTickFrequency())\n",
        "print(label)\n",
        "cv2.putText(img, label, (20, 40), FONT_FACE, FONT_SCALE, RED, THICKNESS, cv2.LINE_AA)\n",
        "\n",
        "cv2.imshow('Output', img)\n",
        "cv2.waitKey(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "riUOtFF55zVE",
        "outputId": "92d748f7-c220-44fc-9132-31f37e3904e7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-2fdf26228c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Give the weight files to the model and load the network using them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mmodelWeights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"YOLOv5/models/yolov5n.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Process image.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/dnn/src/onnx/onnx_importer.cpp:110: error: (-215:Assertion failed) !tensor_proto.raw_data().empty() || !tensor_proto.float_data().empty() || !tensor_proto.double_data().empty() || !tensor_proto.int64_data().empty() in function 'getMatFromTensor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "a = True\n",
        "while a == True:\n",
        "    a = False\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])  \n",
        "\n",
        "    # create transparent overlay for bounding box\n",
        "    bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "    # call our darknet helper on video frame\n",
        "    # detections, width_ratio, height_ratio = darknet_helper(frame, width, height)\n",
        "    run_detector(detector, frame)\n",
        "\n",
        "    # loop through detections and draw them on transparent overlay image\n",
        "    for label, confidence, bbox in detections:\n",
        "      left, top, right, bottom = bbox2points(bbox)\n",
        "      left, top, right, bottom = int(left * width_ratio), int(top * height_ratio), int(right * width_ratio), int(bottom * height_ratio)\n",
        "      bbox_array = cv2.rectangle(bbox_array, (left, top), (right, bottom), class_colors[label], 2)\n",
        "      bbox_array = cv2.putText(bbox_array, \"{} [{:.2f}]\".format(label, float(confidence)),\n",
        "                        (left, top - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
        "                        class_colors[label], 2)\n",
        "      \n",
        "      # get mask\n",
        "      person_detected, mask = get_mask_for_person(img, left, top, right, bottom, label)\n",
        "\n",
        "\n",
        "      # ------------------------  Put Cedric's code here --------------------------\n",
        "      #if person_detected:\n",
        "      # Apply his algo on this image \n",
        "      #  person_cropped = (img*np.expand_dims(mask, axis=2)).astype(int)\n",
        "\n",
        "      # ------------------------  Put Cedric's code here --------------------------\n",
        "\n",
        "    bbox_array[:,:,3] = (bbox_array.max(axis = 2) > 0 ).astype(int) * 255\n",
        "    \n",
        "    # convert overlay of bbox into bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "    # update bbox so next frame gets new overlay\n",
        "    bbox = bbox_bytes"
      ],
      "metadata": {
        "id": "w9_R-i2Z-Jua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}