{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CedricFont/Deep-learning-for-autonomous-vehicle/blob/develop/Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJrRJLBS7DWn"
      },
      "source": [
        "# Softmax exercise\n",
        "\n",
        "We will implement a softmax classifier that is trained on the CIFAR10 dataset. The output is a model that is able to classify the input image into the 10 different classes of the CIFAR10 dataset.\n",
        "\n",
        "Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission.\n",
        "\n",
        "You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- use a validation set to **tune the learning rate** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KQ8b6HtX7DWs"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import pickle\n",
        "from random import shuffle\n",
        "\n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "import platform\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rb1FNWrPvqVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "66c8a633-f3fc-4d45-f8ad-63947edac7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    113\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 135\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    173\u001b[0m   request_id = send_request(\n\u001b[1;32m    174\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 175\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset"
      ],
      "metadata": {
        "id": "pmv_iR7VumNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  datasets.CIFAR10(\n",
        "        root=\"data/\", train=True,\n",
        "        download=True, transform=None,\n",
        "    )\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)    \n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  return Xtr, Ytr\n",
        "\n",
        "    \n",
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'data/cifar-10-batches-py'\n",
        "    X_train, y_train = load_CIFAR10(cifar10_dir)\n",
        "    \n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    \n",
        "    # Normalize the data: subtract the mean image and divide by variance\n",
        "    mean = np.array([0.4914, 0.4822, 0.4465])\n",
        "    std = np.array([0.2023, 0.1994, 0.2010])\n",
        "    X_train = np.divide(np.subtract( X_train/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
        "    X_val = np.divide(np.subtract( X_val/255 , mean[np.newaxis,np.newaxis,:]), std[np.newaxis,np.newaxis,:])\n",
        "\n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "\n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val\n",
        "\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)"
      ],
      "metadata": {
        "id": "vW0orz8Z7O0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "949c0beb776f429a96165c27dec4c917",
            "ce1fca77ce8d4649ab72c779e342368a",
            "f2b6945b319241a2b73449ed4c775021",
            "baff13019edc41558eee22941d06458f",
            "95125e2175184c789568f4572c9bf66c",
            "b5546bfbc4e0449c816f85e8d00e5dd8",
            "5af9f33f344946cc902a56db883c2021",
            "3f5b690d946e4492a15fa75999bc43f7",
            "719fee9dd17d4c2380b0fe29f4e98988",
            "4b575ad92c77468286da76eeb8f6fe17",
            "11aba6ab3eac4ba38848247f5da7d98d"
          ]
        },
        "outputId": "5d8d5de1-343d-4f79-c9b9-062abdd9ca3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "949c0beb776f429a96165c27dec4c917",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data/\n",
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWxBSsLD7DWv"
      },
      "source": [
        "## Softmax Classifier\n",
        "\n",
        "Here, you implement **softmax_loss_vectorized**. This function just returns the loss and gradient after applying the softmax function.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_loss_vectorized(W, X, y):\n",
        "    \"\"\"\n",
        "    Softmax loss function, vectorized version.\n",
        "\n",
        "    Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "    of N examples.\n",
        "\n",
        "    Inputs:\n",
        "    - W: A numpy array of shape (D, C) containing weights.\n",
        "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "    that X[i] has label c, where 0 <= c < C.\n",
        "\n",
        "    Returns a tuple of:\n",
        "    - loss as single float\n",
        "    - gradient with respect to weights W; an array of same shape as W\n",
        "    \"\"\"\n",
        "    # Initialize the loss and gradient to zero.\n",
        "    loss = 0.0\n",
        "    dW = np.zeros_like(W)\n",
        "\n",
        "    #############################################################################\n",
        "    # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "    # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "    # here, it is easy to run into numeric instability.                         #\n",
        "    #############################################################################\n",
        "\n",
        "    C = W.shape[1] # Nb. of classes\n",
        "    f_linear = W.T@X.T # CxD x DxN => CxN\n",
        "    f_linear = f_linear - np.max(f_linear,axis=0) # for more numerically stable solution (exp can overshoot max representable number)\n",
        "    S = np.divide(np.exp(f_linear),np.sum(np.exp(f_linear),axis=0)) # Softmax, CxN\n",
        "    y_one_hot = np.eye(C)[y].T # CxN\n",
        "\n",
        "    loss = np.mean(np.sum(S - y_one_hot,axis=0).T) # Vectorized, Nx1\n",
        "\n",
        "    # The grad of the softmax of each element in the batch w.r.t. w should be \n",
        "    # a matrix of size : DxC\n",
        "    dS_dW = np.multiply(((y_one_hot - S) @ X).T , np.mean(S.T,axis=0)) # NxC x CxN x NxD = NxD\n",
        "    dW = dS_dW\n",
        "\n",
        "    #############################################################################\n",
        "    #                          END OF YOUR CODE                                 #\n",
        "    #############################################################################\n",
        "\n",
        "    return loss, dW"
      ],
      "metadata": {
        "id": "aFed4lnK7yfC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.choice(3,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX36T9m2DqLH",
        "outputId": "1b4ea10d-7ed9-4e6d-b8b1-110c3876bfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 1, 0, 0, 1, 0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This is the Softmax Linear Classifier.\n",
        "### Implement SGD in the train function.\n",
        "### Write the predict function to evaluate the performance on both the training and validation set\n",
        "### Do not touch the loss function. You already defined it above."
      ],
      "metadata": {
        "id": "DtMJHoylvbEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearClassifier(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.W = None\n",
        "\n",
        "    def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "        \"\"\"\n",
        "        Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "            training samples each of dimension D.\n",
        "        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "            means that X[i] has label 0 <= c < C for C classes.\n",
        "        - learning_rate: (float) learning rate for optimization.\n",
        "        - num_iters: (integer) number of steps to take when optimizing\n",
        "        - batch_size: (integer) number of training examples to use at each step.\n",
        "        - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "        Outputs:\n",
        "        A list containing the value of the loss function at each training iteration.\n",
        "        \"\"\"\n",
        "        num_train, dim = X.shape\n",
        "        num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "        if self.W is None:\n",
        "            # lazily initialize W\n",
        "            self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "        # Run stochastic gradient descent to optimize W\n",
        "        loss_history = []\n",
        "        for it in range(num_iters):\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Sample batch_size elements from the training data and their           #\n",
        "            # corresponding labels to use in this round of gradient descent.        #\n",
        "            # Store the data in X_batch and their corresponding labels in           #\n",
        "            # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "            # and y_batch should have shape (batch_size,)                           #\n",
        "            #                                                                       #\n",
        "            # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "            # replacement is faster than sampling without replacement.              #\n",
        "            #########################################################################\n",
        "            choice = np.random.choice(num_train,batch_size)\n",
        "            X_batch = X[choice]\n",
        "            y_batch = y[choice]\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            # evaluate loss and gradient\n",
        "            loss, grad = self.loss(X_batch, y_batch)\n",
        "            loss_history.append(loss)\n",
        "\n",
        "            # perform parameter update\n",
        "            #########################################################################\n",
        "            # TODO:                                                                 #\n",
        "            # Update the weights using the gradient and the learning rate.          #\n",
        "            #########################################################################\n",
        "            self.W -= learning_rate * grad\n",
        "            #########################################################################\n",
        "            #                       END OF YOUR CODE                                #\n",
        "            #########################################################################\n",
        "\n",
        "            if verbose and it % 100 == 0:\n",
        "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "        return loss_history\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Use the trained weights of this linear classifier to predict labels for\n",
        "        data points.\n",
        "\n",
        "        Inputs:\n",
        "        - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "        training samples each of dimension D.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "        array of length N, and each element is an integer giving the predicted\n",
        "        class.\n",
        "        \"\"\"\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        ###########################################################################\n",
        "        # TODO:                                                                   #\n",
        "        # Implement this method. Store the predicted labels in y_pred.            #\n",
        "        ###########################################################################\n",
        "        y_pred = np.argmax(np.divide(np.exp(self.W.T @ X.T),np.sum(np.exp(self.W.T @ X.T),axis=0)),axis=0)\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_pred\n",
        "  \n",
        "    def loss(self, X_batch, y_batch):\n",
        "        \"\"\"\n",
        "        Compute the loss function and its derivative. \n",
        "        Subclasses will override this.\n",
        "\n",
        "        Inputs:\n",
        "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "            data points; each point has dimension D.\n",
        "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "\n",
        "        Returns: A tuple containing:\n",
        "        - loss as a single float\n",
        "        - gradient with respect to self.W; an array of the same shape as W\n",
        "        \"\"\"\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)\n",
        "        "
      ],
      "metadata": {
        "id": "nuhecHcsvJU7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the softmax class is inherited from LinearClassifier and uses the softmax_loss_vectorized function as its loss:"
      ],
      "metadata": {
        "id": "Dzy_rjabwTUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(LinearClassifier):\n",
        "    \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "    def loss(self, X_batch, y_batch):\n",
        "        return softmax_loss_vectorized(self.W, X_batch, y_batch)"
      ],
      "metadata": {
        "id": "GTqa5eihvOvz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "BomGJP517DWv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "32a22cba-ccb6-409b-892d-ac691e0e6908"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-6a6d17e91772>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3073\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloss_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax_loss_vectorized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vectorized loss: %e computed in %fs'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_vectorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-42-18f45965df36>\u001b[0m in \u001b[0;36msoftmax_loss_vectorized\u001b[0;34m(W, X, y)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# The grad of the softmax of each element in the batch w.r.t. w should be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# a matrix of size : DxC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdS_dW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_one_hot\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# NxC x CxN x NxD = NxD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdS_dW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3073,10) (1000,) "
          ]
        }
      ],
      "source": [
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_val, y_val)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized.squeeze(), toc - tic))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = {'a': 1, 'b': 2}\n",
        "min(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me2K3yW8jnnz",
        "outputId": "78f2f655-fd90-41f1-f044-22afc821da0d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "H6Pzyyf47DWw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3627fe-b15b-492d-a8e3-ff0601ce2af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: RuntimeWarning: overflow encountered in exp\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in true_divide\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr 1.000000e-07 train accuracy: 0.093939 val accuracy: 0.093000\n",
            "lr 1.030204e-05 train accuracy: 0.049714 val accuracy: 0.044000\n",
            "lr 2.050408e-05 train accuracy: 0.050184 val accuracy: 0.044000\n",
            "lr 3.070612e-05 train accuracy: 0.049796 val accuracy: 0.044000\n",
            "lr 4.090816e-05 train accuracy: 0.050163 val accuracy: 0.044000\n",
            "lr 5.111020e-05 train accuracy: 0.054796 val accuracy: 0.039000\n",
            "lr 6.131224e-05 train accuracy: 0.056490 val accuracy: 0.054000\n",
            "lr 7.151429e-05 train accuracy: 0.050143 val accuracy: 0.045000\n",
            "lr 8.171633e-05 train accuracy: 0.050184 val accuracy: 0.045000\n",
            "lr 9.191837e-05 train accuracy: 0.050184 val accuracy: 0.045000\n",
            "lr 1.021204e-04 train accuracy: 0.052429 val accuracy: 0.054000\n",
            "lr 1.123224e-04 train accuracy: 0.054673 val accuracy: 0.040000\n",
            "lr 1.225245e-04 train accuracy: 0.057224 val accuracy: 0.054000\n",
            "lr 1.327265e-04 train accuracy: 0.050224 val accuracy: 0.044000\n",
            "lr 1.429286e-04 train accuracy: 0.050184 val accuracy: 0.045000\n",
            "lr 1.531306e-04 train accuracy: 0.057082 val accuracy: 0.047000\n",
            "lr 1.633327e-04 train accuracy: 0.050061 val accuracy: 0.045000\n",
            "lr 1.735347e-04 train accuracy: 0.054939 val accuracy: 0.040000\n",
            "lr 1.837367e-04 train accuracy: 0.050673 val accuracy: 0.044000\n",
            "lr 1.939388e-04 train accuracy: 0.050571 val accuracy: 0.045000\n",
            "lr 2.041408e-04 train accuracy: 0.050327 val accuracy: 0.045000\n",
            "lr 2.143429e-04 train accuracy: 0.050163 val accuracy: 0.045000\n",
            "lr 2.245449e-04 train accuracy: 0.054959 val accuracy: 0.040000\n",
            "lr 2.347469e-04 train accuracy: 0.057041 val accuracy: 0.055000\n",
            "lr 2.449490e-04 train accuracy: 0.050735 val accuracy: 0.044000\n",
            "lr 2.551510e-04 train accuracy: 0.054918 val accuracy: 0.039000\n",
            "lr 2.653531e-04 train accuracy: 0.061265 val accuracy: 0.074000\n",
            "lr 2.755551e-04 train accuracy: 0.054816 val accuracy: 0.040000\n",
            "lr 2.857571e-04 train accuracy: 0.059796 val accuracy: 0.062000\n",
            "lr 2.959592e-04 train accuracy: 0.050327 val accuracy: 0.045000\n",
            "lr 3.061612e-04 train accuracy: 0.050490 val accuracy: 0.045000\n",
            "lr 3.163633e-04 train accuracy: 0.057184 val accuracy: 0.054000\n",
            "lr 3.265653e-04 train accuracy: 0.059306 val accuracy: 0.062000\n",
            "lr 3.367673e-04 train accuracy: 0.050429 val accuracy: 0.045000\n",
            "lr 3.469694e-04 train accuracy: 0.049918 val accuracy: 0.044000\n",
            "lr 3.571714e-04 train accuracy: 0.057184 val accuracy: 0.055000\n",
            "lr 3.673735e-04 train accuracy: 0.052449 val accuracy: 0.053000\n",
            "lr 3.775755e-04 train accuracy: 0.050306 val accuracy: 0.045000\n",
            "lr 3.877776e-04 train accuracy: 0.050347 val accuracy: 0.045000\n",
            "lr 3.979796e-04 train accuracy: 0.057082 val accuracy: 0.055000\n",
            "lr 4.081816e-04 train accuracy: 0.050714 val accuracy: 0.044000\n",
            "lr 4.183837e-04 train accuracy: 0.060939 val accuracy: 0.074000\n",
            "lr 4.285857e-04 train accuracy: 0.059755 val accuracy: 0.059000\n",
            "lr 4.387878e-04 train accuracy: 0.055735 val accuracy: 0.039000\n",
            "lr 4.489898e-04 train accuracy: 0.066714 val accuracy: 0.068000\n",
            "lr 4.591918e-04 train accuracy: 0.052347 val accuracy: 0.054000\n",
            "lr 4.693939e-04 train accuracy: 0.050490 val accuracy: 0.044000\n",
            "lr 4.795959e-04 train accuracy: 0.057673 val accuracy: 0.047000\n",
            "lr 4.897980e-04 train accuracy: 0.055449 val accuracy: 0.039000\n",
            "lr 5.000000e-04 train accuracy: 0.050184 val accuracy: 0.045000\n",
            "best validation accuracy achieved during cross-validation: -1.000000\n"
          ]
        }
      ],
      "source": [
        "# Use the validation set to tune hyperparameters (learning rate). \n",
        "# You should experiment with different ranges for the learning\n",
        "# rates; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = [1e-7, 5e-4]\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning.                                  #\n",
        "# Save the best trained softmax classifer in best_softmax.                     #\n",
        "################################################################################\n",
        "\n",
        "rates = np.linspace(learning_rates[0],learning_rates[1],50)\n",
        "\n",
        "for i,lr in enumerate(rates):\n",
        "  net = LinearClassifier()\n",
        "  net.train(X_train,y_train,learning_rate=lr)\n",
        "  pred_labels_train = net.predict(X_train)\n",
        "  pred_labels_val = net.predict(X_val)\n",
        "  acc_train = np.sum(np.fromiter((1/X_train.shape[0] if pred == y_train[i] else 0 for i,pred in enumerate(pred_labels_train)),float))\n",
        "  acc_val = np.sum(np.fromiter((1/X_val.shape[0] if pred == y_val[i] else 0 for i,pred in enumerate(pred_labels_val)),float))\n",
        "  results[lr] = [acc_train,acc_val]\n",
        "\n",
        "best_softmax = min(results)\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[lr]\n",
        "    print('lr %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "net.W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kM3cj9J2rCHZ",
        "outputId": "afec1ab2-bea6-467c-8686-1dcc161140e3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.72275870e-04, -1.23752688e-03, -7.76890488e-04, ...,\n",
              "         1.57014184e-04, -5.21536259e-04,  1.75778416e-03],\n",
              "       [ 7.80303278e-04, -1.30598190e-03,  8.70657122e-04, ...,\n",
              "         1.57697523e-03,  1.18988241e-03,  4.29675469e-04],\n",
              "       [-3.36842501e-05,  6.46446953e-05, -3.11056619e-04, ...,\n",
              "         9.88123811e-05, -7.26729931e-04, -7.29384304e-04],\n",
              "       ...,\n",
              "       [-1.75634989e-03, -7.66508751e-04, -5.85178553e-04, ...,\n",
              "        -3.98750303e-04, -5.38902401e-04, -1.52701340e-03],\n",
              "       [ 7.12050471e-04, -1.30704146e-04, -6.25491600e-04, ...,\n",
              "         1.24871873e-03,  6.30211823e-04, -9.43985670e-04],\n",
              "       [-1.48218924e-04,  4.25499520e-04, -8.89756139e-04, ...,\n",
              "        -1.78439731e-04, -7.43752690e-04, -7.15002763e-04]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vpj-_mvS7DWx"
      },
      "outputs": [],
      "source": [
        "# evaluate on val set\n",
        "# Evaluate the best softmax on val set\n",
        "y_val_pred = best_softmax.predict(X_val)\n",
        "val_accuracy = np.mean(y_val == y_val_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (val_accuracy, ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI4FL2_a7DWx"
      },
      "source": [
        "Save the best_softmax weights using pickle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4uK5bgB7DWy"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'wb') as f:\n",
        "    pickle.dump(best_softmax.W, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrQpxEbY7DWz"
      },
      "source": [
        "Load the best_softmax weights using pickle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1VkRfIl7DWz"
      },
      "outputs": [],
      "source": [
        "with open('drive/MyDrive/Colab Notebooks/softmax_weights.pkl', 'rb') as f:\n",
        "    W = pickle.load(f)\n",
        "new_softmax = Softmax()\n",
        "new_softmax.W = W.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the code to visualize the weights learned for different classes. Try to notice the interesting patterns."
      ],
      "metadata": {
        "id": "2W7b80F_xkWj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTkVGxZS7DWz"
      },
      "outputs": [],
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oYKmBFJb_Z2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "949c0beb776f429a96165c27dec4c917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ce1fca77ce8d4649ab72c779e342368a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2b6945b319241a2b73449ed4c775021",
              "IPY_MODEL_baff13019edc41558eee22941d06458f",
              "IPY_MODEL_95125e2175184c789568f4572c9bf66c"
            ]
          }
        },
        "ce1fca77ce8d4649ab72c779e342368a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2b6945b319241a2b73449ed4c775021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b5546bfbc4e0449c816f85e8d00e5dd8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5af9f33f344946cc902a56db883c2021"
          }
        },
        "baff13019edc41558eee22941d06458f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3f5b690d946e4492a15fa75999bc43f7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_719fee9dd17d4c2380b0fe29f4e98988"
          }
        },
        "95125e2175184c789568f4572c9bf66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4b575ad92c77468286da76eeb8f6fe17",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:02&lt;00:00, 61218539.74it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11aba6ab3eac4ba38848247f5da7d98d"
          }
        },
        "b5546bfbc4e0449c816f85e8d00e5dd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5af9f33f344946cc902a56db883c2021": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3f5b690d946e4492a15fa75999bc43f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "719fee9dd17d4c2380b0fe29f4e98988": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b575ad92c77468286da76eeb8f6fe17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11aba6ab3eac4ba38848247f5da7d98d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}